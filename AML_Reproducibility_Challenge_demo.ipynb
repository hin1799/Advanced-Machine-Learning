{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hin1799/Advanced-Machine-Learning/blob/main/AML_Reproducibility_Challenge_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Machine Learning\n",
        "\n",
        "# Reproducibility Report on the paper:\n",
        "\n",
        "# **Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers**\n",
        "\n",
        "## Code Implementation\n",
        "\n",
        "**Submitted By**\n",
        "\n",
        "1. Jigar Shekhat (202211004)\n",
        "2. Jainisha Choksi (202211019)\n",
        "3. Hinal Desai (202211035)\n",
        "4. Man Desai (202211040)\n",
        "5. Rutvik Prajapati (202211053)"
      ],
      "metadata": {
        "id": "LVeuIyQW3alI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the code implementation of this paper, GPU is required. Hence we have connected the Google Compute Engine backend from the Google Colab resources."
      ],
      "metadata": {
        "id": "fi-JlthT39sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking if GPU is initialized**"
      ],
      "metadata": {
        "id": "T11ikife4LCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5AubfH4QV9J",
        "outputId": "76ea6f6f-bbba-4a3e-bb23-b291a0b0086c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 25 14:48:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking if cuda is available through PyTorch**"
      ],
      "metadata": {
        "id": "GtJSOrmQ4O29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6pnuPhcQWrY",
        "outputId": "b2a3e666-73a1-4918-c8ed-21fdb99582d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2-ALoAg4vA6",
        "outputId": "7b57c954-1d67-4a58-ebcb-94c36d816ea3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Imports"
      ],
      "metadata": {
        "id": "FW2uNlyc4ucd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Sampler\n",
        "import copy\n",
        "import datetime\n",
        "from termcolor import colored\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "UkS_cFIE4wOz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions\n",
        "\n",
        "The following code contains the helper functions that are required for running the code."
      ],
      "metadata": {
        "id": "AtJsB29b4S4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **to_cuda()** is used to move the tensors stored as values in the dictionary d to the GPU."
      ],
      "metadata": {
        "id": "Ep4dnWca5DFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_cuda(d):\n",
        "    for k, v in d.items():\n",
        "        d[k] = v.cuda()\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "XKgkslJs4YU7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing unnecessary batch dimension from each value in the input dictionary batch."
      ],
      "metadata": {
        "id": "WO8TIprk5csh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def squeeze_batch(batch):\n",
        "    res = {}\n",
        "    for k, v in batch.items():\n",
        "        assert len(v) == 1\n",
        "        res[k] = v[0]\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "-cEybH-C48ee"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to print the training and validation results for a specific epoch."
      ],
      "metadata": {
        "id": "sGIt9O9-6Bk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_res(train_res, val_res, ep):\n",
        "    print((\"epoch {epoch}, train {acc} {train_acc:>7.4f} \"\n",
        "           \"{loss} {train_loss:>10.7f}, {regret} {train_regret:>10.7f} \"\n",
        "           \"val {acc} {val_acc:>10.7f}, {loss} {val_loss:>10.7f}\").format(\n",
        "               epoch=ep,\n",
        "               acc=colored(\"acc\", \"blue\"),\n",
        "               loss=colored(\"loss\", \"yellow\"),\n",
        "               regret=colored(\"regret\", \"red\"),\n",
        "               train_acc=train_res[\"acc\"],\n",
        "               train_loss=train_res[\"loss\"],\n",
        "               train_regret=train_res[\"regret\"],\n",
        "               val_acc=val_res[\"acc\"],\n",
        "               val_loss=val_res[\"loss\"]), flush=True)"
      ],
      "metadata": {
        "id": "RhnNqT4-4_-X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the worst case accuracy of the model's predictions on dataset with attribute-based labels."
      ],
      "metadata": {
        "id": "6oUoCFST6SKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_worst_acc(true, pred, idx, loss, att_idx_dict):\n",
        "    acc_list = (true == pred).float().tolist()\n",
        "    idx = torch.cat(idx).tolist()\n",
        "    idx_origin2new = dict(zip(idx, range(len(idx))))\n",
        "\n",
        "    verbose = True\n",
        "    if len(att_idx_dict) == 1:  # validation\n",
        "        verbose = False\n",
        "\n",
        "    worst_acc_list = []\n",
        "    avg_acc_list = []\n",
        "\n",
        "    for att, data_dict in att_idx_dict.items():\n",
        "        if verbose:\n",
        "            print('{:>20}'.format(att), end=' ')\n",
        "\n",
        "        worst_acc = 1\n",
        "        avg_acc = []\n",
        "        for k, v in data_dict.items():\n",
        "            # value to index mapping\n",
        "            acc = []\n",
        "            for origin in v:\n",
        "                acc.append(acc_list[idx_origin2new[origin]])\n",
        "\n",
        "            if len(acc) > 0:\n",
        "                cur_acc = np.mean(acc)\n",
        "\n",
        "                if verbose:\n",
        "                    print(k, ' {:>8}'.format(len(v)),\n",
        "                          ' {:>7.4f}'.format(cur_acc), end=', ')\n",
        "                if cur_acc < worst_acc:\n",
        "                    worst_acc = cur_acc\n",
        "                avg_acc.append(cur_acc)\n",
        "\n",
        "        if verbose:\n",
        "            print(' worst: {:>7.4f}'.format(worst_acc))\n",
        "\n",
        "        avg_acc = np.mean(avg_acc)\n",
        "        worst_acc_list.append(worst_acc)\n",
        "        avg_acc_list.append(avg_acc)\n",
        "\n",
        "    return {\n",
        "        'acc': np.mean(worst_acc_list),\n",
        "        'avg_acc': np.mean(avg_acc_list),\n",
        "        'loss': loss,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "CAcGwGE-4-XT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "5AFr1LN-6o9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, we have implemented the code for MNIST dataset. So the following code blocks contain the code for Colored MNIST dataset which is an extension of MNIST to make the multi-class classification problem more challenging."
      ],
      "metadata": {
        "id": "dUF7ZM-f6qrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class ColoredMNIST loads the MNIST dataset from PyTorch and creates environments for it.\n",
        "\n",
        "**init**: initializes the dataset. Creates per class data dictionary where key represents the class label and value represents list of samples with that label.\n",
        "\n",
        "Data is split into four environments. Different levels of label corruption for each environment specified by the **envs** dictionary.\n",
        "\n",
        "**make_environment**: creates new environment with given images, labels and label corruption probability. Applies label and color corruption to the data.\n",
        "\n",
        "**get_item**: returns dictionary containing images, labels, and color for given set of indices and environment.\n",
        "\n",
        "**get_all_y** and **get_all_c**: return the label and color values for each environment."
      ],
      "metadata": {
        "id": "Ph5t8mrU6-Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class for ColoredMNIST dataset\n",
        "class ColoredMNIST(Dataset):\n",
        "    def __init__(self, file_path, is_train, val, env0=None, env1=None):\n",
        "        print('Load data')\n",
        "        mnist = datasets.MNIST(file_path, train=is_train, download=True)\n",
        "\n",
        "        print('Create per class data dictionary')\n",
        "\n",
        "        self.data = defaultdict(list)\n",
        "        label_dict = dict(zip(list(range(10)), list(range(10))))\n",
        "\n",
        "        for x, y in zip(mnist.data, mnist.targets):\n",
        "            if int(y) in label_dict:\n",
        "                self.data[label_dict[int(y)]].append(x)\n",
        "\n",
        "        # shuffle data\n",
        "        self.length = 0\n",
        "        random.seed(0)\n",
        "        for k, v in self.data.items():\n",
        "            random.shuffle(v)\n",
        "            self.data[k] = torch.stack(v, dim=0)\n",
        "            self.length += len(self.data[k])\n",
        "\n",
        "        # make environments for each env\n",
        "        # env 0: 0.1, env 1: 0.2, env 3: 0.9\n",
        "        envs = {\n",
        "            0: 0.1,\n",
        "            1: 0.2,\n",
        "            2: -1,\n",
        "            3: 0.9,\n",
        "        }\n",
        "\n",
        "        if val == 'in_domain':  # use train env for validation\n",
        "            envs[2] = envs[1]\n",
        "        else:  # use test env for validation\n",
        "            envs[2] = envs[3]\n",
        "\n",
        "        self.envs = {}\n",
        "        for i in range(4):\n",
        "            images = []\n",
        "            labels = []\n",
        "            for j in range(10):\n",
        "                start = i*(len(self.data[j])//len(envs))\n",
        "                end = (i+1)*(len(self.data[j])//len(envs))\n",
        "\n",
        "                images.append(self.data[j][start:end])\n",
        "                labels.append((torch.ones(end-start) * j).long())\n",
        "\n",
        "            images = torch.cat(images, dim=0)\n",
        "            labels = torch.cat(labels, dim=0)\n",
        "\n",
        "            self.envs[i] = self.make_environment(images, labels, envs[i])\n",
        "\n",
        "        self.data_idx = {\n",
        "            0: self.envs[0]['idx_dict'],\n",
        "            1: self.envs[1]['idx_dict'],\n",
        "            2: self.envs[2]['idx_dict'],\n",
        "            3: self.envs[3]['idx_dict'],\n",
        "        }\n",
        "\n",
        "        # not evaluating worst-case performance of mnist\n",
        "        self.val_att_idx_dict = None\n",
        "        self.test_att_idx_dict = None\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    @staticmethod\n",
        "    def make_environment(images, labels, e):\n",
        "        # different from the IRM repo, here the labels are already binarized\n",
        "        images = images.reshape((-1, 28, 28))\n",
        "\n",
        "        # change label with prob 0.25\n",
        "        prob_label = torch.ones((10, 10)).float() * (0.25 / 9)\n",
        "        for i in range(10):\n",
        "            prob_label[i, i] = 0.75\n",
        "\n",
        "        labels_prob = torch.index_select(prob_label, dim=0, index=labels)\n",
        "        labels = Categorical(probs=labels_prob).sample()\n",
        "\n",
        "        # assign the color variable\n",
        "        prob_color = torch.ones((10, 10)).float() * (e / 9.0)\n",
        "        for i in range(10):\n",
        "            prob_color[i, i] = 1 - e\n",
        "\n",
        "        color_prob = torch.index_select(prob_color, dim=0, index=labels)\n",
        "        color = Categorical(probs=color_prob).sample()\n",
        "\n",
        "        # Apply the color to the image by zeroing out the other color channel\n",
        "        output_images = torch.zeros((len(images), 10, 28, 28))\n",
        "\n",
        "        idx_dict = defaultdict(list)\n",
        "        for i in range(len(images)):\n",
        "            idx_dict[int(labels[i])].append(i)\n",
        "            output_images[i, color[i], :, :] = images[i]\n",
        "\n",
        "        cor = color.float()\n",
        "\n",
        "        idx_list = list(range(len(images)))\n",
        "\n",
        "        return {\n",
        "            'images': (output_images.float() / 255.),\n",
        "            'labels': labels.long(),\n",
        "            'idx_dict': idx_dict,\n",
        "            'idx_list': idx_list,\n",
        "            'cor': cor,\n",
        "        }\n",
        "\n",
        "    def __getitem__(self, keys):\n",
        "        idx = []\n",
        "        if len(keys[0]) == 2:\n",
        "            # without reindexing y\n",
        "            idx = []\n",
        "            for key in keys:\n",
        "                env_id = int(key[1])\n",
        "                idx.append(key[0])\n",
        "\n",
        "            return {\n",
        "                'X': self.envs[env_id]['images'][idx],\n",
        "                'Y': self.envs[env_id]['labels'][idx],\n",
        "                'C': self.envs[env_id]['cor'][idx],\n",
        "                'idx': torch.tensor(idx).long(),\n",
        "            }\n",
        "\n",
        "    def get_all_y(self, env_id):\n",
        "        return self.envs[env_id]['labels'].tolist()\n",
        "\n",
        "    def get_all_c(self, env_id):\n",
        "        return self.envs[env_id]['cor'].tolist()"
      ],
      "metadata": {
        "id": "X94HQYa9616P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ColoredMNIST(file_path='path/to/mnist', is_train=True, val='in_domain')\n",
        "image = dataset[[(0, 0)]]['X'][0][0]\n",
        "\n",
        "pil_image = ToPILImage()(image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "3eom4Lvj8mIy",
        "outputId": "f8e304ad-13ea-4da8-8c13-3d7688cf6f7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load data\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to path/to/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 225137605.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting path/to/mnist/MNIST/raw/train-images-idx3-ubyte.gz to path/to/mnist/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to path/to/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 35203630.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting path/to/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to path/to/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to path/to/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 70148000.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting path/to/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to path/to/mnist/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to path/to/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4420076.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting path/to/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to path/to/mnist/MNIST/raw\n",
            "\n",
            "Create per class data dictionary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and splitting the dataset into train and test."
      ],
      "metadata": {
        "id": "Md9RFDkC-3Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(data_name, val_type):\n",
        "    if data_name == 'MNIST':\n",
        "        train_data = ColoredMNIST('./datasets/mnist', is_train=True,\n",
        "                                  val=val_type)\n",
        "        test_data = ColoredMNIST('./datasets/mnist', is_train=False,\n",
        "                                 val=val_type)\n",
        "\n",
        "        return train_data, test_data\n",
        "\n",
        "train_data, test_data = get_dataset(\"MNIST\", \"in_domain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyL31zVk-1Gd",
        "outputId": "8279a3c1-978f-4e50-ab60-9e8d167f5d40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load data\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 165316237.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/mnist/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 101284024.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 66491283.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/mnist/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 17558091.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/mnist/MNIST/raw\n",
            "\n",
            "Create per class data dictionary\n",
            "Load data\n",
            "Create per class data dictionary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43Tn04Qv-1A_",
        "outputId": "e9e2f840-0ffb-4070-9bde-d273e1c14140"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.ColoredMNIST"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "W1Bv1gX1_B9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code blocks contain the functions required for creating the model using Convolutional Neural Networks and Multi Layer Perceptron."
      ],
      "metadata": {
        "id": "Dgcgr9kl_Dp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "miUMiRP8_h3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, include_fc, hidden_dim):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(10, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(9216, hidden_dim)\n",
        "\n",
        "        self.include_fc = include_fc\n",
        "        if self.include_fc:\n",
        "            self.out_dim = hidden_dim\n",
        "        else:\n",
        "            self.out_dim = 9216\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.view(input.shape[0], 10, 28, 28)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if self.include_fc:\n",
        "            x = self.fc1(x)\n",
        "            x = F.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "5s5mimKa_ANv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the CNN model\n",
        "model = CNN(include_fc=True, hidden_dim=128).cuda()\n",
        "\n",
        "#Displaying the summary for CNN model\n",
        "summary(model, input_size=(1, 10, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkQ98thy_y-E",
        "outputId": "69edea86-9b4d-4943-9aed-9cb668501b42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 26, 26]           2,912\n",
            "            Conv2d-2           [-1, 64, 24, 24]          18,496\n",
            "         Dropout2d-3           [-1, 64, 12, 12]               0\n",
            "            Linear-4                  [-1, 128]       1,179,776\n",
            "================================================================\n",
            "Total params: 1,201,184\n",
            "Trainable params: 1,201,184\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.03\n",
            "Forward/backward pass size (MB): 0.52\n",
            "Params size (MB): 4.58\n",
            "Estimated Total Size (MB): 5.13\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "RsxUFreZ_yc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout, depth=1):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        modules = [nn.Dropout(dropout)]\n",
        "        last_dim = input_dim\n",
        "        for i in range(depth-1):\n",
        "            modules.extend([\n",
        "                nn.Linear(last_dim, hidden_dim),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "            last_dim = hidden_dim\n",
        "        modules.append(nn.Linear(last_dim, output_dim))\n",
        "        self._main = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x, y=None, return_pred=False, grad_penalty=False):\n",
        "        '''\n",
        "            @param x: batch_size * ebd_dim\n",
        "            @param y: batch_size\n",
        "\n",
        "            @return acc\n",
        "            @return loss\n",
        "        '''\n",
        "        logit = self._main(x)\n",
        "\n",
        "        if y is None:\n",
        "            # return prediction directly\n",
        "            return F.log_softmax(logit, dim=-1)\n",
        "\n",
        "        loss = F.cross_entropy(logit, y)\n",
        "\n",
        "        acc = self.compute_acc(logit, y)\n",
        "\n",
        "        if return_pred:\n",
        "            return torch.argmax(logit, dim=1), loss\n",
        "        elif grad_penalty:\n",
        "            # return irm grad penalty\n",
        "            dummy = torch.tensor(1.).cuda().requires_grad_()\n",
        "            loss_tmp = F.cross_entropy(logit * dummy, y)\n",
        "            grad = torch.autograd.grad(loss_tmp, [dummy], create_graph=True)[0]\n",
        "            return acc, loss, torch.sum(grad**2)\n",
        "        else:\n",
        "            return acc, loss\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_acc(pred, true):\n",
        "        '''\n",
        "            Compute the accuracy.\n",
        "            @param pred: batch_size * num_classes\n",
        "            @param true: batch_size\n",
        "        '''\n",
        "        return torch.mean((torch.argmax(pred, dim=1) == true).float()).item()"
      ],
      "metadata": {
        "id": "MO6En4KQAwDw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(input_dim=784, hidden_dim=128, output_dim=10, dropout=0.1, depth=2).cuda()\n",
        "\n",
        "summary(model, (784,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l83xVqG4Ayd-",
        "outputId": "84807836-d3db-4572-d4eb-c40d3a5c464b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Dropout-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 128]         100,480\n",
            "           Dropout-3                  [-1, 128]               0\n",
            "              ReLU-4                  [-1, 128]               0\n",
            "            Linear-5                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.39\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the model to use"
      ],
      "metadata": {
        "id": "2PQOJmK9Hj4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(dataset, method):\n",
        "  model = {}\n",
        "  if dataset == \"MNIST\":\n",
        "    model['ebd'] = CNN(include_fc=True, hidden_dim=390).cuda()\n",
        "    \n",
        "  if method in ['irm', 'erm', 'dro', 'ours', 'oracle']:\n",
        "    model['clf_all'] = MLP(out_dim, 390, num_classes,\n",
        "                               dropout, depth=1).cuda()\n",
        "\n",
        "    opt = torch.optim.Adam(list(model['ebd'].parameters()) +\n",
        "                               list(model['clf_all'].parameters()), lr=lr,\n",
        "                               weight_decay=weight_decay)\n",
        "\n",
        "    return model, opt\n",
        "  \n",
        "  elif method in ['rgm']:\n",
        "      # RGM learns environment-specific classifiers together with the overall classifier\n",
        "      model['clf_all'] = MLP(out_dim, hidden_dim, num_classes,\n",
        "                               dropout, depth=1).cuda()\n",
        "\n",
        "      model['clf_0'] = MLP(out_dim, hidden_dim, num_classes,\n",
        "                             dropout, depth=2).cuda()\n",
        "\n",
        "      model['clf_1'] = MLP(out_dim, hidden_dim, num_classes,\n",
        "                             dropout, depth=2).cuda()\n",
        "\n",
        "      opt_all = torch.optim.Adam(list(model['ebd'].parameters()) +\n",
        "                               list(model['clf_all'].parameters()), lr=lr,\n",
        "                               weight_decay=weight_decay)\n",
        "\n",
        "      opt_0 = torch.optim.Adam(model['clf_0'].parameters(),\n",
        "                                 lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "      opt_1 = torch.optim.Adam(model['clf_1'].parameters(),\n",
        "                                 lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "      return model, [opt_all, opt_0, opt_1]"
      ],
      "metadata": {
        "id": "crED0At1Hlh2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Sampling"
      ],
      "metadata": {
        "id": "NgirDZg9BLVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EnvSampler is a custom implementation of PyTorch sampler which is used to sample data from a dataset. It shuffles index list and then samples through data returning the indices and environment information in a yield statement."
      ],
      "metadata": {
        "id": "j8wKFFvuBa-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvSampler(Sampler):\n",
        "    def __init__(self, num_batches, batch_size, env_id, idx_list, seed=0):\n",
        "        self.num_batches = num_batches\n",
        "        self.batch_size = batch_size\n",
        "        self.env_id = env_id\n",
        "        self.idx_list = idx_list\n",
        "\n",
        "        random.seed(seed)\n",
        "\n",
        "        if self.num_batches == -1:\n",
        "            self.length = ((len(self.idx_list) + self.batch_size - 1) // self.batch_size)\n",
        "\n",
        "        else:\n",
        "            self.length = self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.num_batches == -1:\n",
        "            # iterate through the dataset sequentially\n",
        "            # for testing\n",
        "            random.shuffle(self.idx_list)\n",
        "\n",
        "            # sample through the data\n",
        "            for i in range(self.length):\n",
        "                start = i * self.batch_size\n",
        "                end = min((i+1) * self.batch_size, len(self.idx_list))\n",
        "\n",
        "                # provide the idx and the env information to the dataset\n",
        "                yield [(idx, self.env_id) for idx in self.idx_list[start:end]]\n",
        "\n",
        "        else:\n",
        "            for _ in range(self.num_batches):\n",
        "                if self.batch_size < len(self.idx_list):\n",
        "                    yield [(idx, self.env_id) for idx in\n",
        "                           random.sample(self.idx_list, self.batch_size)]\n",
        "                else:\n",
        "                    # if the number of examples is less than a batch\n",
        "                    yield [(idx, self.env_id) for idx in self.idx_list]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "metadata": {
        "id": "ywQ7jStVBNS5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model parameters"
      ],
      "metadata": {
        "id": "adnkGtjWJJHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These parameters could be specified as arguments when running the code in terminal. But here we need to explicitly specify them."
      ],
      "metadata": {
        "id": "NuUqA5RGJMbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the parameters\n",
        "\n",
        "dropout = 0.1\n",
        "lr=0.001 \n",
        "weight_decay=0.001\n",
        "out_dim=390\n",
        "num_classes = 10\n",
        "epochs=100\n",
        "hidden_dim=390\n",
        "num_batches=100\n",
        "batch_size=50\n",
        "num_epochs=100\n",
        "patience=20\n",
        "l_regret=1\n",
        "anneal_iters=1"
      ],
      "metadata": {
        "id": "jW9OurRtBNa6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different Baselines"
      ],
      "metadata": {
        "id": "mYeSbSjVKKx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code to save test accuracy and loss for each method\n",
        "\n",
        "dict_acc_values = {} # to save the accuracy for each method\n",
        "dict_loss_values = {} #to save the loss for each method"
      ],
      "metadata": {
        "id": "ydNXEfj2K5A-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ERM"
      ],
      "metadata": {
        "id": "2asCmvnPUZvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_erm_loop(train_loaders, model, opt, ep):\n",
        "    stats = {}\n",
        "    for k in ['acc', 'loss', 'regret', 'loss_train']:\n",
        "        stats[k] = []\n",
        "\n",
        "    step = 0\n",
        "    for batch_0, batch_1 in zip(train_loaders[0], train_loaders[1]):\n",
        "        # work on each batch\n",
        "        # sample from the two env equally\n",
        "        model['ebd'].train()\n",
        "        model['clf_all'].train()\n",
        "\n",
        "        batch_0 = to_cuda(squeeze_batch(batch_0))\n",
        "        batch_1 = to_cuda(squeeze_batch(batch_1))\n",
        "\n",
        "        x_0 = model['ebd'](batch_0['X'])\n",
        "        y_0 = batch_0['Y']\n",
        "        x_1 = model['ebd'](batch_1['X'])\n",
        "        y_1 = batch_1['Y']\n",
        "\n",
        "        acc_0, loss_0 = model['clf_all'](x_0, y_0, return_pred=False,\n",
        "                                               grad_penalty=False)\n",
        "\n",
        "        acc_1, loss_1 = model['clf_all'](x_1, y_1, return_pred=False,\n",
        "                                               grad_penalty=False)\n",
        "\n",
        "        loss = (loss_0 + loss_1) / 2.0\n",
        "        acc = (acc_0 + acc_1) / 2.0\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        stats['acc'].append(acc)\n",
        "        stats['loss'].append(loss.item())\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        stats[k] = float(np.mean(np.array(v)))\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "7BtH44rGOTcB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_erm_loop(test_loader, model, ep, att_idx_dict=None):\n",
        "    loss_list = []\n",
        "    true, pred = [], []\n",
        "\n",
        "    if att_idx_dict is not None:\n",
        "        idx = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        # work on each batch\n",
        "        model['ebd'].eval()\n",
        "        model['clf_all'].eval()\n",
        "\n",
        "        batch = to_cuda(squeeze_batch(batch))\n",
        "\n",
        "        x = model['ebd'](batch['X'])\n",
        "        y = batch['Y']\n",
        "\n",
        "        y_hat, loss = model['clf_all'](x, y, return_pred=True)\n",
        "\n",
        "        true.append(y)\n",
        "        pred.append(y_hat)\n",
        "\n",
        "        if att_idx_dict is not None:\n",
        "            idx.append(batch['idx'])\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    true = torch.cat(true)\n",
        "    pred = torch.cat(pred)\n",
        "\n",
        "    acc = torch.mean((true == pred).float()).item()\n",
        "    loss = np.mean(np.array(loss_list))\n",
        "\n",
        "    if att_idx_dict is not None:\n",
        "        return get_worst_acc(true, pred, idx, loss, att_idx_dict)\n",
        "\n",
        "    return {\n",
        "        'acc': acc,\n",
        "        'loss': loss,\n",
        "    }"
      ],
      "metadata": {
        "id": "AHMqH0IsPDdd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def erm(train_data, test_data, model, opt):\n",
        "    train_loaders = []\n",
        "    for i in range(2):\n",
        "        train_loaders.append(DataLoader(\n",
        "            train_data,\n",
        "            sampler=EnvSampler(num_batches, batch_size, i,\n",
        "                               train_data.envs[i]['idx_list']),\n",
        "        num_workers=7))\n",
        "\n",
        "    test_loaders = []\n",
        "    for i in range(4):\n",
        "        test_loaders.append(DataLoader(\n",
        "            test_data,\n",
        "            sampler=EnvSampler(-1, batch_size, i,\n",
        "                               test_data.envs[i]['idx_list']),\n",
        "        num_workers=7))\n",
        "\n",
        "    # start training\n",
        "    best_acc = -1\n",
        "    best_val_res = None\n",
        "    best_model = {}\n",
        "    cycle = 0\n",
        "    for ep in range(num_epochs):\n",
        "        train_res = train_erm_loop(train_loaders, model, opt, ep)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # validation\n",
        "            val_res = test_erm_loop(test_loaders[2], model, ep,\n",
        "                                test_data.val_att_idx_dict)\n",
        "\n",
        "        print_res(train_res, val_res, ep)\n",
        "\n",
        "        if min(train_res['acc'], val_res['acc']) > best_acc:\n",
        "            best_acc = min(train_res['acc'], val_res['acc'])\n",
        "            best_val_res = val_res\n",
        "            best_train_res = train_res\n",
        "            cycle = 0\n",
        "            # save best ebd\n",
        "            for k in 'ebd', 'clf_all':\n",
        "                best_model[k] = copy.deepcopy(model[k].state_dict())\n",
        "        else:\n",
        "            cycle += 1\n",
        "\n",
        "        if cycle == patience:\n",
        "            break\n",
        "\n",
        "    # load best model\n",
        "    for k in 'ebd', 'clf_all':\n",
        "        model[k].load_state_dict(best_model[k])\n",
        "\n",
        "    # get the results\n",
        "    test_res = test_erm_loop(test_loaders[3], model, ep,\n",
        "                         test_data.test_att_idx_dict)\n",
        "    print('Best train')\n",
        "    print(train_res)\n",
        "    print('Best val')\n",
        "    val_res = best_val_res\n",
        "    print(val_res)\n",
        "    print('Test')\n",
        "    print(test_res)\n",
        "\n",
        "    return train_res, val_res, test_res"
      ],
      "metadata": {
        "id": "n7SMtVj3PRyW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model(\"MNIST\", \"erm\")"
      ],
      "metadata": {
        "id": "9uY2Kz6eOiGq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V2beYkGPy1I",
        "outputId": "97421721-d179-4a95-e1ba-cfd16869dae8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ebd': CNN(\n",
              "   (conv1): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "   (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "   (dropout1): Dropout2d(p=0.25, inplace=False)\n",
              "   (dropout2): Dropout2d(p=0.5, inplace=False)\n",
              "   (fc1): Linear(in_features=9216, out_features=390, bias=True)\n",
              " ),\n",
              " 'clf_all': MLP(\n",
              "   (_main): Sequential(\n",
              "     (0): Dropout(p=0.1, inplace=False)\n",
              "     (1): Linear(in_features=390, out_features=10, bias=True)\n",
              "   )\n",
              " )}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running the model**"
      ],
      "metadata": {
        "id": "6cacc1TxKldm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_res, val_res, test_res = erm(train_data, test_data, model, opt)"
      ],
      "metadata": {
        "id": "Oz-wIsP0RDDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b71a4b-9a03-46bc-f2f0-5a58057b1fc4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 7 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train acc  0.7946 loss  0.8420285, regret        nan val acc  0.7973568, loss  0.7875747\n",
            "epoch 1, train acc  0.8328 loss  0.6260106, regret        nan val acc  0.7957549, loss  0.7372116\n",
            "epoch 2, train acc  0.8327 loss  0.6114619, regret        nan val acc  0.8009611, loss  0.7188423\n",
            "epoch 3, train acc  0.8454 loss  0.5746185, regret        nan val acc  0.8033640, loss  0.7108138\n",
            "epoch 4, train acc  0.8437 loss  0.5791313, regret        nan val acc  0.7989588, loss  0.6944131\n",
            "epoch 5, train acc  0.8357 loss  0.5729103, regret        nan val acc  0.8049660, loss  0.7011563\n",
            "epoch 6, train acc  0.8382 loss  0.5754800, regret        nan val acc  0.8009611, loss  0.6852501\n",
            "epoch 7, train acc  0.8341 loss  0.5691570, regret        nan val acc  0.7949539, loss  0.6874888\n",
            "epoch 8, train acc  0.8350 loss  0.5692989, regret        nan val acc  0.7913496, loss  0.6877835\n",
            "epoch 9, train acc  0.8373 loss  0.5540304, regret        nan val acc  0.8013616, loss  0.6736101\n",
            "epoch 10, train acc  0.8359 loss  0.5737084, regret        nan val acc  0.7977573, loss  0.6907402\n",
            "epoch 11, train acc  0.8445 loss  0.5276591, regret        nan val acc  0.8001602, loss  0.7079576\n",
            "epoch 12, train acc  0.8401 loss  0.5424193, regret        nan val acc  0.7997597, loss  0.6610521\n",
            "epoch 13, train acc  0.8323 loss  0.5571320, regret        nan val acc  0.7901482, loss  0.6698151\n",
            "epoch 14, train acc  0.8364 loss  0.5316093, regret        nan val acc  0.8057669, loss  0.6755818\n",
            "epoch 15, train acc  0.8401 loss  0.5393476, regret        nan val acc  0.7969564, loss  0.6553555\n",
            "epoch 16, train acc  0.8467 loss  0.5155370, regret        nan val acc  0.8017621, loss  0.6742270\n",
            "epoch 17, train acc  0.8454 loss  0.5219960, regret        nan val acc  0.7973568, loss  0.6718452\n",
            "epoch 18, train acc  0.8410 loss  0.5271859, regret        nan val acc  0.7977573, loss  0.6462477\n",
            "epoch 19, train acc  0.8370 loss  0.5380607, regret        nan val acc  0.7893472, loss  0.6517532\n",
            "epoch 20, train acc  0.8446 loss  0.5044586, regret        nan val acc  0.7969564, loss  0.6903314\n",
            "epoch 21, train acc  0.8426 loss  0.5355696, regret        nan val acc  0.7981578, loss  0.6769804\n",
            "epoch 22, train acc  0.8406 loss  0.5104251, regret        nan val acc  0.8037645, loss  0.7011515\n",
            "epoch 23, train acc  0.8496 loss  0.4903577, regret        nan val acc  0.7941530, loss  0.6561182\n",
            "epoch 24, train acc  0.8432 loss  0.5104750, regret        nan val acc  0.7973568, loss  0.6615770\n",
            "epoch 25, train acc  0.8408 loss  0.5211968, regret        nan val acc  0.8061674, loss  0.6434331\n",
            "epoch 26, train acc  0.8398 loss  0.5065016, regret        nan val acc  0.7985583, loss  0.6738668\n",
            "epoch 27, train acc  0.8387 loss  0.5127487, regret        nan val acc  0.7989588, loss  0.6482099\n",
            "epoch 28, train acc  0.8508 loss  0.4835489, regret        nan val acc  0.8017621, loss  0.6519500\n",
            "epoch 29, train acc  0.8450 loss  0.4943845, regret        nan val acc  0.7985583, loss  0.6556423\n",
            "epoch 30, train acc  0.8504 loss  0.4852216, regret        nan val acc  0.8013616, loss  0.6541074\n",
            "epoch 31, train acc  0.8438 loss  0.4893750, regret        nan val acc  0.7965559, loss  0.6560797\n",
            "epoch 32, train acc  0.8363 loss  0.5036320, regret        nan val acc  0.7909492, loss  0.6485792\n",
            "epoch 33, train acc  0.8431 loss  0.5009764, regret        nan val acc  0.7897477, loss  0.6702191\n",
            "epoch 34, train acc  0.8460 loss  0.4838163, regret        nan val acc  0.8029636, loss  0.6790002\n",
            "epoch 35, train acc  0.8450 loss  0.5039707, regret        nan val acc  0.7941530, loss  0.6418865\n",
            "epoch 36, train acc  0.8426 loss  0.4999501, regret        nan val acc  0.7973568, loss  0.6591567\n",
            "epoch 37, train acc  0.8447 loss  0.4877315, regret        nan val acc  0.7969564, loss  0.6567953\n",
            "epoch 38, train acc  0.8419 loss  0.4843388, regret        nan val acc  0.7889467, loss  0.6493534\n",
            "epoch 39, train acc  0.8459 loss  0.4835781, regret        nan val acc  0.7937525, loss  0.6447316\n",
            "epoch 40, train acc  0.8355 loss  0.5087107, regret        nan val acc  0.7909492, loss  0.6391793\n",
            "epoch 41, train acc  0.8484 loss  0.4791091, regret        nan val acc  0.7961554, loss  0.6416097\n",
            "epoch 42, train acc  0.8486 loss  0.4800192, regret        nan val acc  0.8029636, loss  0.6735993\n",
            "epoch 43, train acc  0.8450 loss  0.4817466, regret        nan val acc  0.7945535, loss  0.6390395\n",
            "epoch 44, train acc  0.8415 loss  0.4795464, regret        nan val acc  0.7969564, loss  0.6481431\n",
            "epoch 45, train acc  0.8527 loss  0.4582613, regret        nan val acc  0.8009611, loss  0.6628674\n",
            "Best train\n",
            "{'acc': 0.8526999771595001, 'loss': 0.4582613128423691, 'regret': nan, 'loss_train': nan}\n",
            "Best val\n",
            "{'acc': 0.8061674237251282, 'loss': 0.6434331046044827}\n",
            "Test\n",
            "{'acc': 0.19743692874908447, 'loss': 2.4162051916122436}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ORFUjgJPyPB",
        "outputId": "44bcf414-cf34-4595-dcfd-791ac2906d00"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': 0.8526999771595001,\n",
              " 'loss': 0.4582613128423691,\n",
              " 'regret': nan,\n",
              " 'loss_train': nan}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X67wIrLPz-t",
        "outputId": "040a3995-e041-4cb2-8ba0-c1e70e4274c8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': 0.8061674237251282, 'loss': 0.6434331046044827}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNkC3y3fP12S",
        "outputId": "97a32ae8-7209-4b74-ce5d-a9de95366fdd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': 0.19743692874908447, 'loss': 2.4162051916122436}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_acc_values['erm'] = test_res['acc']"
      ],
      "metadata": {
        "id": "847K_BboKonp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_loss_values['erm'] = test_res['loss']"
      ],
      "metadata": {
        "id": "cT0eHilqLgKx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. IRM"
      ],
      "metadata": {
        "id": "EDcgcPJ2P4zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_irm_loop(train_loaders, model, opt, ep):\n",
        "    stats = {}\n",
        "    for k in ['acc', 'loss', 'regret', 'loss_train']:\n",
        "        stats[k] = []\n",
        "\n",
        "    step = ep * num_batches\n",
        "    for batch_0, batch_1 in zip(train_loaders[0], train_loaders[1]):\n",
        "        # work on each batch\n",
        "        model['ebd'].train()\n",
        "        model['clf_all'].train()\n",
        "\n",
        "        batch_0 = to_cuda(squeeze_batch(batch_0))\n",
        "        batch_1 = to_cuda(squeeze_batch(batch_1))\n",
        "\n",
        "        x_0 = model['ebd'](batch_0['X'])\n",
        "        y_0 = batch_0['Y']\n",
        "        x_1 = model['ebd'](batch_1['X'])\n",
        "        y_1 = batch_1['Y']\n",
        "\n",
        "        acc_0, loss_0, grad_0 = model['clf_all'](x_0, y_0, return_pred=False,\n",
        "                                               grad_penalty=True)\n",
        "\n",
        "        acc_1, loss_1, grad_1 = model['clf_all'](x_1, y_1, return_pred=False,\n",
        "                                               grad_penalty=True)\n",
        "\n",
        "        loss_ce = (loss_0 + loss_1) / 2.0\n",
        "        regret = (grad_0 + grad_1) / 2.0\n",
        "\n",
        "        acc = (acc_0 + acc_1) / 2.0\n",
        "\n",
        "        weight = l_regret if step > anneal_iters else 1.0\n",
        "\n",
        "        loss_total = loss_ce + weight * regret\n",
        "\n",
        "        if weight > 1.0:\n",
        "            loss_total /= weight\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss_total.backward()\n",
        "        opt.step()\n",
        "\n",
        "        stats['acc'].append(acc)\n",
        "        stats['loss'].append(loss_total.item())\n",
        "        stats['loss_train'].append(loss_ce.item())\n",
        "        stats['regret'].append(regret.item())\n",
        "        step += 1\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        stats[k] = float(np.mean(np.array(v)))\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "4ns6EfUEP6ru"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_irm_loop(test_loader, model, ep, att_idx_dict=None):\n",
        "    loss_list = []\n",
        "    true, pred = [], []\n",
        "    if att_idx_dict is not None:\n",
        "        idx = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        # work on each batch\n",
        "        model['ebd'].eval()\n",
        "        model['clf_all'].eval()\n",
        "\n",
        "        batch = to_cuda(squeeze_batch(batch))\n",
        "\n",
        "        x = model['ebd'](batch['X'])\n",
        "        y = batch['Y']\n",
        "\n",
        "        y_hat, loss = model['clf_all'](x, y, return_pred=True)\n",
        "\n",
        "        true.append(y)\n",
        "        pred.append(y_hat)\n",
        "\n",
        "        if att_idx_dict is not None:\n",
        "            idx.append(batch['idx'])\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    true = torch.cat(true)\n",
        "    pred = torch.cat(pred)\n",
        "\n",
        "    acc = torch.mean((true == pred).float()).item()\n",
        "    loss = np.mean(np.array(loss_list))\n",
        "\n",
        "    if att_idx_dict is not None:\n",
        "        return get_worst_acc(true, pred, idx, loss, att_idx_dict)\n",
        "\n",
        "    return {\n",
        "        'acc': acc,\n",
        "        'loss': loss,\n",
        "    }"
      ],
      "metadata": {
        "id": "hNoehoNGL6vd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def irm(train_data, test_data, model, opt):\n",
        "    train_loaders = []\n",
        "    for i in range(2):\n",
        "        train_loaders.append(DataLoader(\n",
        "            train_data,\n",
        "            sampler=EnvSampler(num_batches, batch_size, i,\n",
        "                               train_data.envs[i]['idx_list']),\n",
        "        num_workers=7))\n",
        "\n",
        "    test_loaders = []\n",
        "    for i in range(4):\n",
        "        test_loaders.append(DataLoader(\n",
        "            test_data,\n",
        "            sampler=EnvSampler(-1, batch_size, i,\n",
        "                               test_data.envs[i]['idx_list']),\n",
        "        num_workers=7))\n",
        "\n",
        "    # start training\n",
        "    best_acc = -1\n",
        "    best_val_res = None\n",
        "    best_model = {}\n",
        "    cycle = 0\n",
        "    for ep in range(num_epochs):\n",
        "        train_res = train_irm_loop(train_loaders, model, opt, ep)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # validation\n",
        "            val_res = test_irm_loop(test_loaders[2], model, ep,\n",
        "                                test_data.val_att_idx_dict)\n",
        "\n",
        "        print_res(train_res, val_res, ep)\n",
        "\n",
        "        if min(train_res['acc'], val_res['acc']) > best_acc:\n",
        "            best_acc = min(train_res['acc'], val_res['acc'])\n",
        "            best_val_res = val_res\n",
        "            best_train_res = train_res\n",
        "            cycle = 0\n",
        "            # save best ebd\n",
        "            for k in 'ebd', 'clf_all':\n",
        "                best_model[k] = copy.deepcopy(model[k].state_dict())\n",
        "        else:\n",
        "            cycle += 1\n",
        "\n",
        "        if cycle == patience:\n",
        "            break\n",
        "\n",
        "    # load best model\n",
        "    for k in 'ebd', 'clf_all':\n",
        "        model[k].load_state_dict(best_model[k])\n",
        "\n",
        "    # get the results\n",
        "    test_res = test_irm_loop(test_loaders[3], model, ep,\n",
        "                         test_data.test_att_idx_dict)\n",
        "\n",
        "    print('Best train')\n",
        "    print(train_res)\n",
        "    print('Best val')\n",
        "    val_res = best_val_res\n",
        "    print(val_res)\n",
        "    print('Test')\n",
        "    print(test_res)\n",
        "\n",
        "    return train_res, val_res, test_res"
      ],
      "metadata": {
        "id": "EhbT08vtL_LP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model(\"MNIST\", \"irm\")"
      ],
      "metadata": {
        "id": "5VD3T13qXQhM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_res, val_res, test_res = irm(train_data, test_data, model, opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ffb-IyuQeGh",
        "outputId": "a701beb6-76be-4b13-efbe-091831444393"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train acc  0.7749 loss  0.9210680, regret  0.0559625 val acc  0.7949539, loss  0.7843181\n",
            "epoch 1, train acc  0.8302 loss  0.6737439, regret  0.0406183 val acc  0.7813376, loss  0.7465635\n",
            "epoch 2, train acc  0.8296 loss  0.6590167, regret  0.0409010 val acc  0.7933520, loss  0.7232595\n",
            "epoch 3, train acc  0.8351 loss  0.6215533, regret  0.0366573 val acc  0.7925511, loss  0.7130435\n",
            "epoch 4, train acc  0.8366 loss  0.6297162, regret  0.0367420 val acc  0.7997597, loss  0.7059960\n",
            "epoch 5, train acc  0.8301 loss  0.6225981, regret  0.0339641 val acc  0.7977573, loss  0.7167129\n",
            "epoch 6, train acc  0.8319 loss  0.6201463, regret  0.0304009 val acc  0.8001602, loss  0.6847670\n",
            "epoch 7, train acc  0.8303 loss  0.6144218, regret  0.0307712 val acc  0.7925511, loss  0.6794125\n",
            "epoch 8, train acc  0.8287 loss  0.6078326, regret  0.0300436 val acc  0.7889467, loss  0.7043716\n",
            "epoch 9, train acc  0.8325 loss  0.6204539, regret  0.0454268 val acc  0.7933520, loss  0.6901113\n",
            "epoch 10, train acc  0.8306 loss  0.6213283, regret  0.0329272 val acc  0.7933520, loss  0.6785826\n",
            "epoch 11, train acc  0.8397 loss  0.5710618, regret  0.0336925 val acc  0.7933520, loss  0.6711637\n",
            "epoch 12, train acc  0.8366 loss  0.5889748, regret  0.0324577 val acc  0.7961554, loss  0.6601049\n",
            "epoch 13, train acc  0.8327 loss  0.5887402, regret  0.0320120 val acc  0.7933520, loss  0.6684891\n",
            "epoch 14, train acc  0.8383 loss  0.5665142, regret  0.0262553 val acc  0.8021626, loss  0.6868897\n",
            "epoch 15, train acc  0.8376 loss  0.5780373, regret  0.0286466 val acc  0.7881458, loss  0.6681988\n",
            "epoch 16, train acc  0.8447 loss  0.5544746, regret  0.0259376 val acc  0.7981578, loss  0.6964972\n",
            "epoch 17, train acc  0.8411 loss  0.5627001, regret  0.0298132 val acc  0.8001602, loss  0.6703268\n",
            "epoch 18, train acc  0.8386 loss  0.5632766, regret  0.0290394 val acc  0.7961554, loss  0.6617437\n",
            "epoch 19, train acc  0.8346 loss  0.5769967, regret  0.0361328 val acc  0.7853424, loss  0.6739635\n",
            "epoch 20, train acc  0.8443 loss  0.5501356, regret  0.0315773 val acc  0.7929515, loss  0.6928606\n",
            "epoch 21, train acc  0.8401 loss  0.5723514, regret  0.0314203 val acc  0.7965559, loss  0.6880427\n",
            "epoch 22, train acc  0.8382 loss  0.5391177, regret  0.0234585 val acc  0.8005607, loss  0.7083123\n",
            "epoch 23, train acc  0.8452 loss  0.5212129, regret  0.0222576 val acc  0.7985583, loss  0.6624641\n",
            "epoch 24, train acc  0.8431 loss  0.5393666, regret  0.0258873 val acc  0.7973568, loss  0.6643685\n",
            "epoch 25, train acc  0.8372 loss  0.5499826, regret  0.0263523 val acc  0.7981578, loss  0.6474672\n",
            "epoch 26, train acc  0.8399 loss  0.5346181, regret  0.0284100 val acc  0.8009611, loss  0.6832727\n",
            "epoch 27, train acc  0.8377 loss  0.5502214, regret  0.0292975 val acc  0.7829396, loss  0.6620483\n",
            "epoch 28, train acc  0.8480 loss  0.5103124, regret  0.0199786 val acc  0.7989588, loss  0.6383594\n",
            "epoch 29, train acc  0.8422 loss  0.5338887, regret  0.0279244 val acc  0.7929515, loss  0.6443075\n",
            "epoch 30, train acc  0.8478 loss  0.5043291, regret  0.0218635 val acc  0.8017621, loss  0.6796449\n",
            "epoch 31, train acc  0.8438 loss  0.5158286, regret  0.0234298 val acc  0.7953544, loss  0.6700583\n",
            "epoch 32, train acc  0.8356 loss  0.5254828, regret  0.0224645 val acc  0.7925511, loss  0.6631211\n",
            "epoch 33, train acc  0.8380 loss  0.5262577, regret  0.0234481 val acc  0.7929515, loss  0.6904052\n",
            "epoch 34, train acc  0.8422 loss  0.5107315, regret  0.0235018 val acc  0.8005607, loss  0.6762157\n",
            "Best train\n",
            "{'acc': 0.8421999761462211, 'loss': 0.5107314765453339, 'regret': 0.023501802693426724, 'loss_train': 0.4872296726703644}\n",
            "Best val\n",
            "{'acc': 0.8021625876426697, 'loss': 0.6868896824121475}\n",
            "Test\n",
            "{'acc': 0.17140568792819977, 'loss': 2.5557396364212037}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_acc_values['irm'] = test_res['acc']"
      ],
      "metadata": {
        "id": "hEEH-1s2LoV3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_loss_values['irm'] = test_res['loss']"
      ],
      "metadata": {
        "id": "Ir7VebiaLv-B"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. DRO"
      ],
      "metadata": {
        "id": "x_5VC4k7Q5ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dro_loop(train_loaders, model, opt, ep):\n",
        "    stats = {}\n",
        "    for k in ['worst_loss', 'avg_loss', 'worst_acc', 'avg_acc']:\n",
        "        stats[k] = []\n",
        "\n",
        "    step = 0\n",
        "    for batches in zip(*train_loaders):\n",
        "        # work on each batch\n",
        "        model['ebd'].train()\n",
        "        model['clf_all'].train()\n",
        "\n",
        "        x, y = [], []\n",
        "\n",
        "        for batch in batches:\n",
        "            batch = to_cuda(squeeze_batch(batch))\n",
        "            x.append(batch['X'])\n",
        "            y.append(batch['Y'])\n",
        "\n",
        "        if dataset in ['beer_0', 'beer_1', 'beer_2', 'pubmed']:\n",
        "            # text models have varying length between batches\n",
        "            pred = []\n",
        "            for cur_x in x:\n",
        "                pred.append(model['clf_all'](model['ebd'](cur_x)))\n",
        "            pred = torch.cat(pred, dim=0)\n",
        "        else:\n",
        "            pred = model['clf_all'](model['ebd'](torch.cat(x, dim=0)))\n",
        "        cur_idx = 0\n",
        "\n",
        "        avg_loss = 0\n",
        "        avg_acc = 0\n",
        "        worst_loss = 0\n",
        "        worst_acc = 0\n",
        "\n",
        "        for cur_true in y:\n",
        "            cur_pred = pred[cur_idx:cur_idx+len(cur_true)]\n",
        "            cur_idx += len(cur_true)\n",
        "\n",
        "            loss = F.cross_entropy(cur_pred, cur_true)\n",
        "            acc = torch.mean((torch.argmax(cur_pred, dim=1) == cur_true).float()).item()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "            avg_acc += acc\n",
        "\n",
        "            if loss.item() > worst_loss:\n",
        "                worst_loss = loss\n",
        "                worst_acc = acc\n",
        "\n",
        "        opt.zero_grad()\n",
        "        worst_loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        avg_loss /= len(y)\n",
        "        avg_acc /= len(y)\n",
        "\n",
        "        stats['avg_acc'].append(avg_acc)\n",
        "        stats['avg_loss'].append(avg_loss)\n",
        "        stats['worst_acc'].append(worst_acc)\n",
        "        stats['worst_loss'].append(worst_loss.item())\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        stats[k] = float(np.mean(np.array(v)))\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "2_GoZDHRMi6Z"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dro_loop(test_loader, model, ep, args, att_idx_dict=None):\n",
        "    loss_list = []\n",
        "    true, pred = [], []\n",
        "    if att_idx_dict is not None:\n",
        "        idx = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        # work on each batch\n",
        "        model['ebd'].eval()\n",
        "        model['clf_all'].eval()\n",
        "\n",
        "        batch = to_cuda(squeeze_batch(batch))\n",
        "\n",
        "        x = model['ebd'](batch['X'])\n",
        "        y = batch['Y']\n",
        "\n",
        "        y_hat, loss = model['clf_all'](x, y, return_pred=True)\n",
        "\n",
        "        true.append(y)\n",
        "        pred.append(y_hat)\n",
        "\n",
        "        if att_idx_dict is not None:\n",
        "            idx.append(batch['idx'])\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    true = torch.cat(true)\n",
        "    pred = torch.cat(pred)\n",
        "\n",
        "    acc = torch.mean((true == pred).float()).item()\n",
        "    loss = np.mean(np.array(loss_list))\n",
        "\n",
        "    if att_idx_dict is not None:\n",
        "        return get_worst_acc(true, pred, idx, loss, att_idx_dict)\n",
        "\n",
        "    return {\n",
        "        'acc': acc,\n",
        "        'loss': loss,\n",
        "    }"
      ],
      "metadata": {
        "id": "x-MA_iZGMo5C"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_res(train_res, val_res, ep):\n",
        "    print((\"epoch {epoch}, train {acc} {train_acc:>7.4f} {train_worst_acc:>7.4f} \"\n",
        "           \"{loss} {train_loss:>10.7f} {train_worst_loss:>10.7f} \"\n",
        "           \"val {acc} {val_acc:>10.7f}, {loss} {val_loss:>10.7f}\").format(\n",
        "               epoch=ep,\n",
        "               acc=colored(\"acc\", \"blue\"),\n",
        "               loss=colored(\"loss\", \"yellow\"),\n",
        "               regret=colored(\"regret\", \"red\"),\n",
        "               train_acc=train_res[\"avg_acc\"],\n",
        "               train_worst_acc=train_res[\"worst_acc\"],\n",
        "               train_loss=train_res[\"avg_loss\"],\n",
        "               train_worst_loss=train_res[\"worst_loss\"],\n",
        "               val_acc=val_res[\"acc\"],\n",
        "               val_loss=val_res[\"loss\"]), flush=True)"
      ],
      "metadata": {
        "id": "vGqcMleiMslS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"mnist\"\n",
        "\n",
        "def dro(train_data, test_data, model, opt):\n",
        "    # define groups by env-label pair\n",
        "    train_loaders = []\n",
        "    for env in range(2):\n",
        "        # look at each environment, each label\n",
        "        groups = {}\n",
        "        label_list = train_data.get_all_y(env)\n",
        "        for idx, label in zip(train_data.envs[env]['idx_list'], label_list):\n",
        "            if label not in groups:\n",
        "                groups[label] = [idx]\n",
        "            else:\n",
        "                groups[label].append(idx)\n",
        "\n",
        "        for group in groups.values():\n",
        "            train_loaders.append(DataLoader(\n",
        "                train_data,\n",
        "                sampler=EnvSampler(num_batches, batch_size, env,\n",
        "                                   group),\n",
        "            num_workers=2))\n",
        "\n",
        "    test_loaders = []\n",
        "    for i in range(4):\n",
        "        test_loaders.append(DataLoader(\n",
        "            test_data,\n",
        "            sampler=EnvSampler(-1, batch_size, i,\n",
        "                               test_data.envs[i]['idx_list']), num_workers=2))\n",
        "\n",
        "    # start training\n",
        "    best_acc = -1\n",
        "    best_val_res = None\n",
        "    best_model = {}\n",
        "    cycle = 0\n",
        "    for ep in range(num_epochs):\n",
        "        train_res = train_dro_loop(train_loaders, model, opt, ep)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # validation\n",
        "            val_res = test_dro_loop(test_loaders[2], model, ep,\n",
        "                                test_data.val_att_idx_dict)\n",
        "\n",
        "        print_res(train_res, val_res, ep)\n",
        "\n",
        "        if min(train_res['worst_acc'], val_res['acc']) > best_acc:\n",
        "            best_acc = min(train_res['worst_acc'], val_res['acc'])\n",
        "            best_val_res = val_res\n",
        "            best_train_res = train_res\n",
        "            cycle = 0\n",
        "            # save best ebd\n",
        "            for k in 'ebd', 'clf_all':\n",
        "                best_model[k] = copy.deepcopy(model[k].state_dict())\n",
        "        else:\n",
        "            cycle += 1\n",
        "\n",
        "        if cycle == patience:\n",
        "            break\n",
        "\n",
        "    # load best model\n",
        "    for k in 'ebd', 'clf_all':\n",
        "        model[k].load_state_dict(best_model[k])\n",
        "\n",
        "    # get the results\n",
        "    test_res = test_dro_loop(test_loaders[3], model, ep,\n",
        "                         test_data.test_att_idx_dict)\n",
        "    val_res = best_val_res\n",
        "    print('Best train')\n",
        "    print(train_res)\n",
        "    print('Best val')\n",
        "    print(val_res)\n",
        "    print('Test')\n",
        "    print(test_res)\n",
        "\n",
        "    return train_res, val_res, test_res\n"
      ],
      "metadata": {
        "id": "M6RGu_zHQh4D"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model(\"MNIST\", \"dro\")"
      ],
      "metadata": {
        "id": "EMI4-SJsYLJX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_res, val_res, test_res = dro(train_data, test_data, model, opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5UgwPh3P6m9",
        "outputId": "cbc6c5b6-b68c-44aa-d4eb-30ba408dad8f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train acc  0.3436  0.1472 loss  2.2659991  2.3084410 val acc  0.6131358, loss  2.1430915\n",
            "epoch 1, train acc  0.6476  0.4896 loss  1.3770184  1.7695391 val acc  0.7989588, loss  0.9615998\n",
            "epoch 2, train acc  0.7238  0.5860 loss  1.0282237  1.4825930 val acc  0.7973568, loss  0.8853149\n",
            "epoch 3, train acc  0.7532  0.6052 loss  0.9375852  1.4028953 val acc  0.7937525, loss  0.8621928\n",
            "epoch 4, train acc  0.7723  0.6398 loss  0.8957944  1.3459052 val acc  0.7937525, loss  0.8105501\n",
            "epoch 5, train acc  0.7814  0.6656 loss  0.8507567  1.2720304 val acc  0.7985583, loss  0.7882036\n",
            "epoch 6, train acc  0.7753  0.6392 loss  0.8332347  1.2375741 val acc  0.7869443, loss  0.7828395\n",
            "epoch 7, train acc  0.7792  0.6484 loss  0.8055851  1.2142837 val acc  0.7949539, loss  0.7745895\n",
            "epoch 8, train acc  0.7848  0.6600 loss  0.7815849  1.2014073 val acc  0.7893472, loss  0.7664323\n",
            "epoch 9, train acc  0.7794  0.6344 loss  0.7735641  1.1814263 val acc  0.7849419, loss  0.7748727\n",
            "epoch 10, train acc  0.7844  0.6550 loss  0.7445641  1.1338399 val acc  0.7945535, loss  0.7481900\n",
            "epoch 11, train acc  0.7990  0.6818 loss  0.7215044  1.0988396 val acc  0.7949539, loss  0.7304519\n",
            "epoch 12, train acc  0.8031  0.6996 loss  0.7089585  1.0736347 val acc  0.7905487, loss  0.7135786\n",
            "epoch 13, train acc  0.8041  0.6946 loss  0.6966549  1.0666835 val acc  0.7937525, loss  0.7186573\n",
            "epoch 14, train acc  0.8072  0.6890 loss  0.6763239  1.0405198 val acc  0.8005607, loss  0.6926023\n",
            "epoch 15, train acc  0.8071  0.6854 loss  0.6775052  1.0619220 val acc  0.7897477, loss  0.7039140\n",
            "epoch 16, train acc  0.8052  0.6802 loss  0.6774062  1.0740788 val acc  0.7949539, loss  0.7147022\n",
            "epoch 17, train acc  0.8120  0.6984 loss  0.6597116  1.0174098 val acc  0.7933520, loss  0.7133434\n",
            "epoch 18, train acc  0.8126  0.6878 loss  0.6573169  1.0332985 val acc  0.7949539, loss  0.6994861\n",
            "epoch 19, train acc  0.8134  0.6906 loss  0.6424766  1.0168736 val acc  0.7905487, loss  0.7016637\n",
            "epoch 20, train acc  0.8131  0.6936 loss  0.6385983  0.9949496 val acc  0.7961554, loss  0.6812517\n",
            "epoch 21, train acc  0.8144  0.7018 loss  0.6324441  0.9931348 val acc  0.7937525, loss  0.6843494\n",
            "epoch 22, train acc  0.8148  0.6950 loss  0.6303960  0.9967478 val acc  0.7949539, loss  0.6765031\n",
            "epoch 23, train acc  0.8176  0.6998 loss  0.6268297  0.9786835 val acc  0.7901482, loss  0.7043272\n",
            "epoch 24, train acc  0.8211  0.7052 loss  0.6087052  0.9618319 val acc  0.7969564, loss  0.6873549\n",
            "epoch 25, train acc  0.8177  0.6982 loss  0.6157481  0.9745608 val acc  0.7941530, loss  0.6681496\n",
            "epoch 26, train acc  0.8162  0.6986 loss  0.6182757  0.9656185 val acc  0.7889467, loss  0.6921408\n",
            "epoch 27, train acc  0.8201  0.6984 loss  0.6050389  0.9735782 val acc  0.7865438, loss  0.6777844\n",
            "epoch 28, train acc  0.8182  0.7096 loss  0.5965303  0.9270445 val acc  0.7913496, loss  0.6854781\n",
            "epoch 29, train acc  0.8214  0.7008 loss  0.5927200  0.9559386 val acc  0.7945535, loss  0.6781381\n",
            "epoch 30, train acc  0.8230  0.7130 loss  0.5871543  0.9235408 val acc  0.7941530, loss  0.6830546\n",
            "epoch 31, train acc  0.8215  0.7096 loss  0.5895063  0.9248549 val acc  0.7901482, loss  0.6809846\n",
            "epoch 32, train acc  0.8240  0.7146 loss  0.5830000  0.9245526 val acc  0.7861434, loss  0.6780151\n",
            "epoch 33, train acc  0.8209  0.7040 loss  0.5876887  0.9344624 val acc  0.7869443, loss  0.6821043\n",
            "epoch 34, train acc  0.8240  0.7180 loss  0.5726715  0.9053356 val acc  0.7949539, loss  0.6695388\n",
            "epoch 35, train acc  0.8285  0.7206 loss  0.5650821  0.8960546 val acc  0.7873448, loss  0.6877111\n",
            "epoch 36, train acc  0.8275  0.7192 loss  0.5678899  0.9006294 val acc  0.7825391, loss  0.6701326\n",
            "epoch 37, train acc  0.8278  0.7170 loss  0.5713655  0.9072687 val acc  0.7933520, loss  0.6761494\n",
            "epoch 38, train acc  0.8276  0.7248 loss  0.5590320  0.8707141 val acc  0.7909492, loss  0.6663347\n",
            "epoch 39, train acc  0.8271  0.7138 loss  0.5602894  0.8874152 val acc  0.7893472, loss  0.6671237\n",
            "epoch 40, train acc  0.8305  0.7224 loss  0.5559570  0.8731436 val acc  0.7921506, loss  0.6719409\n",
            "epoch 41, train acc  0.8330  0.7296 loss  0.5465786  0.8477001 val acc  0.7913496, loss  0.6667047\n",
            "epoch 42, train acc  0.8302  0.7208 loss  0.5498501  0.8697811 val acc  0.7969564, loss  0.6682764\n",
            "epoch 43, train acc  0.8334  0.7284 loss  0.5486666  0.8755434 val acc  0.7925511, loss  0.6691826\n",
            "epoch 44, train acc  0.8309  0.7222 loss  0.5501471  0.8696697 val acc  0.7913496, loss  0.6764538\n",
            "epoch 45, train acc  0.8309  0.7176 loss  0.5492851  0.8806887 val acc  0.7921506, loss  0.6963913\n",
            "epoch 46, train acc  0.8331  0.7170 loss  0.5399110  0.8661609 val acc  0.7873448, loss  0.6778639\n",
            "epoch 47, train acc  0.8343  0.7286 loss  0.5347928  0.8293659 val acc  0.7845415, loss  0.6922787\n",
            "epoch 48, train acc  0.8340  0.7214 loss  0.5361740  0.8627956 val acc  0.7917501, loss  0.6685094\n",
            "epoch 49, train acc  0.8354  0.7228 loss  0.5337161  0.8551905 val acc  0.7897477, loss  0.6859138\n",
            "epoch 50, train acc  0.8348  0.7262 loss  0.5294493  0.8396809 val acc  0.7897477, loss  0.6692561\n",
            "epoch 51, train acc  0.8368  0.7368 loss  0.5215388  0.8114984 val acc  0.7921506, loss  0.6696010\n",
            "epoch 52, train acc  0.8343  0.7276 loss  0.5300575  0.8231821 val acc  0.7909492, loss  0.6738108\n",
            "epoch 53, train acc  0.8347  0.7260 loss  0.5253403  0.8321596 val acc  0.7853424, loss  0.6848241\n",
            "epoch 54, train acc  0.8373  0.7330 loss  0.5128503  0.8072296 val acc  0.7953544, loss  0.6709803\n",
            "epoch 55, train acc  0.8359  0.7270 loss  0.5256853  0.8392547 val acc  0.7817381, loss  0.6742108\n",
            "epoch 56, train acc  0.8376  0.7358 loss  0.5174128  0.8198879 val acc  0.7917501, loss  0.6715498\n",
            "epoch 57, train acc  0.8390  0.7288 loss  0.5145600  0.8095659 val acc  0.7845415, loss  0.6730897\n",
            "epoch 58, train acc  0.8360  0.7296 loss  0.5100575  0.7968706 val acc  0.7941530, loss  0.6693513\n",
            "epoch 59, train acc  0.8361  0.7302 loss  0.5188135  0.8199094 val acc  0.7797357, loss  0.6685227\n",
            "epoch 60, train acc  0.8404  0.7422 loss  0.5048817  0.7956838 val acc  0.7945535, loss  0.6665106\n",
            "epoch 61, train acc  0.8410  0.7400 loss  0.5027891  0.7918314 val acc  0.7929515, loss  0.6800890\n",
            "epoch 62, train acc  0.8408  0.7398 loss  0.5021103  0.7956155 val acc  0.7913496, loss  0.6761383\n",
            "epoch 63, train acc  0.8380  0.7424 loss  0.5067507  0.7976502 val acc  0.7969564, loss  0.6635248\n",
            "epoch 64, train acc  0.8420  0.7382 loss  0.4978529  0.7826711 val acc  0.7889467, loss  0.6803421\n",
            "epoch 65, train acc  0.8390  0.7284 loss  0.5008880  0.7969484 val acc  0.7813376, loss  0.6846455\n",
            "epoch 66, train acc  0.8403  0.7376 loss  0.5005073  0.7865327 val acc  0.7873448, loss  0.6713173\n",
            "epoch 67, train acc  0.8426  0.7418 loss  0.4997736  0.7904332 val acc  0.7905487, loss  0.6686684\n",
            "epoch 68, train acc  0.8445  0.7514 loss  0.4894724  0.7565863 val acc  0.7869443, loss  0.6777830\n",
            "epoch 69, train acc  0.8424  0.7440 loss  0.4909956  0.7777061 val acc  0.7897477, loss  0.6706793\n",
            "epoch 70, train acc  0.8423  0.7334 loss  0.4922446  0.7893741 val acc  0.7897477, loss  0.6704191\n",
            "epoch 71, train acc  0.8434  0.7446 loss  0.4887475  0.7727740 val acc  0.7909492, loss  0.6726466\n",
            "epoch 72, train acc  0.8436  0.7430 loss  0.4824358  0.7704769 val acc  0.7817381, loss  0.6936718\n",
            "epoch 73, train acc  0.8432  0.7532 loss  0.4882153  0.7541237 val acc  0.7969564, loss  0.6634337\n",
            "epoch 74, train acc  0.8450  0.7412 loss  0.4792997  0.7597262 val acc  0.7933520, loss  0.6779043\n",
            "epoch 75, train acc  0.8475  0.7388 loss  0.4754303  0.7628942 val acc  0.7885463, loss  0.6818698\n",
            "epoch 76, train acc  0.8447  0.7488 loss  0.4801385  0.7634922 val acc  0.7893472, loss  0.6717382\n",
            "epoch 77, train acc  0.8448  0.7488 loss  0.4780548  0.7487320 val acc  0.7845415, loss  0.6846820\n",
            "epoch 78, train acc  0.8454  0.7424 loss  0.4722321  0.7491933 val acc  0.7877453, loss  0.6704138\n",
            "epoch 79, train acc  0.8476  0.7456 loss  0.4720758  0.7525969 val acc  0.7885463, loss  0.6823819\n",
            "epoch 80, train acc  0.8478  0.7488 loss  0.4713595  0.7587562 val acc  0.7853424, loss  0.6735243\n",
            "epoch 81, train acc  0.8459  0.7548 loss  0.4647298  0.7234396 val acc  0.7837405, loss  0.6943515\n",
            "epoch 82, train acc  0.8481  0.7426 loss  0.4602487  0.7397481 val acc  0.7837405, loss  0.6841827\n",
            "epoch 83, train acc  0.8486  0.7526 loss  0.4644579  0.7305697 val acc  0.7793352, loss  0.6819642\n",
            "epoch 84, train acc  0.8468  0.7462 loss  0.4686643  0.7578603 val acc  0.7921506, loss  0.6698327\n",
            "epoch 85, train acc  0.8490  0.7474 loss  0.4583773  0.7353056 val acc  0.7941530, loss  0.6709385\n",
            "epoch 86, train acc  0.8470  0.7480 loss  0.4639150  0.7272995 val acc  0.7837405, loss  0.6781472\n",
            "epoch 87, train acc  0.8496  0.7542 loss  0.4575651  0.7194153 val acc  0.7865438, loss  0.6659384\n",
            "epoch 88, train acc  0.8482  0.7554 loss  0.4558556  0.7198178 val acc  0.7853424, loss  0.6624998\n",
            "epoch 89, train acc  0.8492  0.7582 loss  0.4527208  0.7199986 val acc  0.7929515, loss  0.6735123\n",
            "epoch 90, train acc  0.8506  0.7468 loss  0.4519995  0.7284805 val acc  0.7845415, loss  0.6768413\n",
            "epoch 91, train acc  0.8513  0.7510 loss  0.4489238  0.7081755 val acc  0.7797357, loss  0.6737585\n",
            "epoch 92, train acc  0.8514  0.7504 loss  0.4452285  0.7090904 val acc  0.7845415, loss  0.6806127\n",
            "epoch 93, train acc  0.8522  0.7520 loss  0.4468545  0.7137640 val acc  0.7901482, loss  0.6796606\n",
            "epoch 94, train acc  0.8528  0.7576 loss  0.4479792  0.7136296 val acc  0.7845415, loss  0.6784206\n",
            "epoch 95, train acc  0.8524  0.7528 loss  0.4488914  0.7150847 val acc  0.7865438, loss  0.6874751\n",
            "epoch 96, train acc  0.8553  0.7582 loss  0.4390992  0.6952488 val acc  0.7813376, loss  0.6858735\n",
            "epoch 97, train acc  0.8532  0.7602 loss  0.4421247  0.7094411 val acc  0.7853424, loss  0.6870918\n",
            "epoch 98, train acc  0.8542  0.7552 loss  0.4395997  0.7065031 val acc  0.7865438, loss  0.6742146\n",
            "epoch 99, train acc  0.8534  0.7496 loss  0.4376172  0.6991113 val acc  0.7885463, loss  0.6887686\n",
            "Best train\n",
            "{'worst_loss': 0.699111288189888, 'avg_loss': 0.4376171986050904, 'worst_acc': 0.7495999842882156, 'avg_acc': 0.8534499761760236}\n",
            "Best val\n",
            "{'acc': 0.7853423953056335, 'loss': 0.6870918053388596}\n",
            "Test\n",
            "{'acc': 0.24789747595787048, 'loss': 2.1978156423568724}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_acc_values['dro'] = test_res['acc']"
      ],
      "metadata": {
        "id": "yNUW-7OmL3xC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_loss_values['dro'] = test_res['loss']"
      ],
      "metadata": {
        "id": "ZcnwepDHL5IZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Oracle"
      ],
      "metadata": {
        "id": "IAiqNicHSF-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dro_loop(train_loaders, model, opt, ep):\n",
        "    stats = {}\n",
        "    for k in ['worst_loss', 'avg_loss', 'worst_acc', 'avg_acc']:\n",
        "        stats[k] = []\n",
        "\n",
        "    step = 0\n",
        "    for batches in zip(*train_loaders):\n",
        "        # work on each batch\n",
        "        model['ebd'].train()\n",
        "        model['clf_all'].train()\n",
        "\n",
        "        x, y = [], []\n",
        "\n",
        "        for batch in batches:\n",
        "            batch = to_cuda(squeeze_batch(batch))\n",
        "            x.append(batch['X'])\n",
        "            y.append(batch['Y'])\n",
        "\n",
        "        if dataset in ['beer_0', 'beer_1', 'beer_2']:\n",
        "            # text models have varying length between batches\n",
        "            pred = []\n",
        "            for cur_x in x:\n",
        "                pred.append(model['clf_all'](model['ebd'](cur_x)))\n",
        "            pred = torch.cat(pred, dim=0)\n",
        "        else:\n",
        "            pred = model['clf_all'](model['ebd'](torch.cat(x, dim=0)))\n",
        "\n",
        "        cur_idx = 0\n",
        "\n",
        "        avg_loss = 0\n",
        "        avg_acc = 0\n",
        "        worst_loss = 0\n",
        "        worst_acc = 0\n",
        "\n",
        "        for cur_true in y:\n",
        "            cur_pred = pred[cur_idx:cur_idx+len(cur_true)]\n",
        "            cur_idx += len(cur_true)\n",
        "\n",
        "            loss = F.cross_entropy(cur_pred, cur_true)\n",
        "            acc = torch.mean((torch.argmax(cur_pred, dim=1) == cur_true).float()).item()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "            avg_acc += acc\n",
        "\n",
        "            if loss.item() > worst_loss:\n",
        "                worst_loss = loss\n",
        "                worst_acc = acc\n",
        "\n",
        "        opt.zero_grad()\n",
        "        worst_loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        avg_loss /= len(y)\n",
        "        avg_acc /= len(y)\n",
        "\n",
        "        stats['avg_acc'].append(avg_acc)\n",
        "        stats['avg_loss'].append(avg_loss)\n",
        "        stats['worst_acc'].append(worst_acc)\n",
        "        stats['worst_loss'].append(worst_loss.item())\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        stats[k] = float(np.mean(np.array(v)))\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "Y691Ig81P6Mj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(test_loader, model, ep,return_idx=False):\n",
        "    loss_list = []\n",
        "    true, pred, cor = [], [], []\n",
        "    if return_idx:\n",
        "        idx = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        # work on each batch\n",
        "        model['ebd'].eval()\n",
        "        model['clf_all'].eval()\n",
        "\n",
        "        batch = to_cuda(squeeze_batch(batch))\n",
        "\n",
        "        x = model['ebd'](batch['X'])\n",
        "        y = batch['Y']\n",
        "        c = batch['C']\n",
        "\n",
        "        y_hat, loss = model['clf_all'](x, y, return_pred=True)\n",
        "\n",
        "        true.append(y)\n",
        "        pred.append(y_hat)\n",
        "        cor.append(c)\n",
        "        if return_idx:\n",
        "            idx.append(batch['idx'])\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    true = torch.cat(true)\n",
        "    pred = torch.cat(pred)\n",
        "\n",
        "    acc = torch.mean((true == pred).float()).item()\n",
        "    loss = np.mean(np.array(loss_list))\n",
        "\n",
        "    if not return_idx:\n",
        "        return {\n",
        "            'acc': acc,\n",
        "            'loss': loss,\n",
        "        }\n",
        "    else:\n",
        "        cor = torch.cat(cor).tolist()\n",
        "        true = true.tolist()\n",
        "        pred = pred.tolist()\n",
        "        idx = torch.cat(idx).tolist()\n",
        "\n",
        "        # split correct and wrong idx\n",
        "        correct_idx, wrong_idx = [], []\n",
        "\n",
        "        # compute correlation between cor and y for analysis\n",
        "        correct_cor, wrong_cor = [], []\n",
        "        correct_y, wrong_y = [], []\n",
        "\n",
        "        for i, y, y_hat, c in zip(idx, true, pred, cor):\n",
        "            if y == y_hat:\n",
        "                correct_idx.append(i)\n",
        "                # correct_cor += (1 if (int(c) == int(y)) else 0)\n",
        "                correct_cor.append(c)\n",
        "                correct_y.append(y)\n",
        "            else:\n",
        "                wrong_idx.append(i)\n",
        "                # wrong_cor += (1 if (int(c) == int(y)) else 0)\n",
        "                wrong_cor.append(c)\n",
        "                wrong_y.append(y)\n",
        "\n",
        "        return {\n",
        "            'acc': acc,\n",
        "            'loss': loss,\n",
        "            'correct_idx': correct_idx,\n",
        "            'correct_cor': correct_cor,\n",
        "            'correct_y': correct_y,\n",
        "            'wrong_idx': wrong_idx,\n",
        "            'wrong_cor': wrong_cor,\n",
        "            'wrong_y': wrong_y,\n",
        "        }"
      ],
      "metadata": {
        "id": "z61fz8SgNYiO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_res(train_res, val_res, ep):\n",
        "    print((\"epoch {epoch}, train {acc} {train_acc:>7.4f} {train_worst_acc:>7.4f} \"\n",
        "           \"{loss} {train_loss:>10.7f} {train_worst_loss:>10.7f} \"\n",
        "           \"val {acc} {val_acc:>10.7f}, {loss} {val_loss:>10.7f}\").format(\n",
        "               epoch=ep,\n",
        "               acc=colored(\"acc\", \"blue\"),\n",
        "               loss=colored(\"loss\", \"yellow\"),\n",
        "               regret=colored(\"regret\", \"red\"),\n",
        "               train_acc=train_res[\"avg_acc\"],\n",
        "               train_worst_acc=train_res[\"worst_acc\"],\n",
        "               train_loss=train_res[\"avg_loss\"],\n",
        "               train_worst_loss=train_res[\"worst_loss\"],\n",
        "               val_acc=val_res[\"acc\"],\n",
        "               val_loss=val_res[\"loss\"]), flush=True)\n",
        "\n",
        "\n",
        "def print_group_stats(pretrain_res):\n",
        "    for i in range(len(pretrain_res)):\n",
        "        print('{}on{}_correct '.format(1-i, i), end='')\n",
        "        print('len: {:7d}, correlation: {:>7.4f}'.format(\n",
        "            len(pretrain_res[i]['correct_idx']),\n",
        "            np.corrcoef(pretrain_res[i]['correct_cor'],\n",
        "                        pretrain_res[i]['correct_y'])[0,1]))\n",
        "\n",
        "        print('{}on{}_wrong   '.format(1-i, i), end='')\n",
        "        print('len: {:7d}, correlation: {:>7.4f}'.format(\n",
        "            len(pretrain_res[i]['wrong_idx']),\n",
        "            np.corrcoef(pretrain_res[i]['wrong_cor'],\n",
        "                        pretrain_res[i]['wrong_y'])[0,1]))\n",
        "\n",
        "\n",
        "def print_pretrain_res(train_res, test_res, ep, i):\n",
        "    print((\"petrain {i}, epoch {epoch}, train {acc} {train_acc:>7.4f} \"\n",
        "           \"{loss} {train_loss:>7.4f}, \"\n",
        "           \"val {acc} {test_acc:>7.4f}, {loss} {test_loss:>7.4f} \").format(\n",
        "               epoch=ep,\n",
        "               i = i,\n",
        "               acc=colored(\"acc\", \"blue\"),\n",
        "               loss=colored(\"loss\", \"yellow\"),\n",
        "               ebd=colored(\"ebd\", \"red\"),\n",
        "               train_acc=train_res[\"acc\"],\n",
        "               train_loss=train_res[\"loss\"],\n",
        "               test_acc=test_res[\"acc\"],\n",
        "               test_loss=test_res[\"loss\"]), flush=True)"
      ],
      "metadata": {
        "id": "qDuejr9XNd9t"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oracle(train_data, test_data, model, opt):\n",
        "    # oracle has access to the spurious attribute\n",
        "    # can use this to define groups\n",
        "    train_loaders = []\n",
        "    for env in range(2):\n",
        "        groups = [[], []]\n",
        "        label_list = train_data.get_all_y(env)\n",
        "        cor_list = train_data.get_all_c(env)\n",
        "        for idx in range(len(label_list)):\n",
        "            label = label_list[idx]\n",
        "            cor = cor_list[idx]\n",
        "            if label == cor:\n",
        "                groups[0].append(idx)\n",
        "            else:\n",
        "                groups[1].append(idx)\n",
        "\n",
        "        for group in groups:\n",
        "            train_loaders.append(DataLoader(\n",
        "                train_data,\n",
        "                sampler=EnvSampler(num_batches, batch_size, env,\n",
        "                                   group), num_workers=5))\n",
        "\n",
        "    test_loaders = []\n",
        "    for i in range(4):\n",
        "        test_loaders.append(DataLoader(\n",
        "            test_data,\n",
        "            sampler=EnvSampler(-1, batch_size, i,\n",
        "                               test_data.envs[i]['idx_list']), num_workers=5))\n",
        "\n",
        "    # start training\n",
        "    best_acc = -1\n",
        "    best_val_res = None\n",
        "    best_model = {}\n",
        "    cycle = 0\n",
        "    for ep in range(num_epochs):\n",
        "        train_res = train_dro_loop(train_loaders, model, opt, ep)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # validation\n",
        "            val_res = test_loop(test_loaders[2], model, ep)\n",
        "\n",
        "        print_res(train_res, val_res, ep)\n",
        "\n",
        "        if min(train_res['worst_acc'], val_res['acc']) > best_acc:\n",
        "            best_acc = min(train_res['worst_acc'], val_res['acc'])\n",
        "            best_val_res = val_res\n",
        "            best_train_res = train_res\n",
        "            cycle = 0\n",
        "            # save best ebd\n",
        "            for k in 'ebd', 'clf_all':\n",
        "                best_model[k] = copy.deepcopy(model[k].state_dict())\n",
        "        else:\n",
        "            cycle += 1\n",
        "\n",
        "        if cycle == patience:\n",
        "            break\n",
        "\n",
        "    # load best model\n",
        "    for k in 'ebd', 'clf_all':\n",
        "        model[k].load_state_dict(best_model[k])\n",
        "\n",
        "    # get the results\n",
        "    test_res = test_loop(test_loaders[3], model, ep)\n",
        "    print('Best train')\n",
        "    print(train_res)\n",
        "    print('Best val')\n",
        "    val_res = best_val_res\n",
        "    print(val_res)\n",
        "    print('Test')\n",
        "    print(test_res)\n",
        "\n",
        "    return train_res, val_res, test_res"
      ],
      "metadata": {
        "id": "-rzRBf5bNjNw"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model(\"MNIST\", \"oracle\")"
      ],
      "metadata": {
        "id": "XactHBt2Yv5V"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_res, val_res, test_res = oracle(train_data, test_data, model, opt)"
      ],
      "metadata": {
        "id": "ll1fnfSuSwRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9cc744c-b4b9-4d75-e1e3-680b408c1c8f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train acc  0.5223  0.4570 loss  1.6529220  1.8214202 val acc  0.6868242, loss  1.2566948\n",
            "epoch 1, train acc  0.6546  0.5976 loss  1.3637999  1.5667677 val acc  0.7048458, loss  1.1929178\n",
            "epoch 2, train acc  0.6796  0.6172 loss  1.2790103  1.5011268 val acc  0.7080497, loss  1.1831268\n",
            "epoch 3, train acc  0.6976  0.6390 loss  1.2063759  1.4183332 val acc  0.7120545, loss  1.1077625\n",
            "epoch 4, train acc  0.6988  0.6414 loss  1.2095925  1.4237130 val acc  0.7240689, loss  1.1070165\n",
            "epoch 5, train acc  0.7063  0.6440 loss  1.1696175  1.3811240 val acc  0.7180617, loss  1.1268220\n",
            "epoch 6, train acc  0.7123  0.6556 loss  1.1365345  1.3446258 val acc  0.7304766, loss  1.0505850\n",
            "epoch 7, train acc  0.7156  0.6630 loss  1.1056671  1.3095142 val acc  0.7200641, loss  1.0978509\n",
            "epoch 8, train acc  0.7225  0.6610 loss  1.0643108  1.2800322 val acc  0.7344814, loss  1.0175445\n",
            "epoch 9, train acc  0.7254  0.6750 loss  1.0454279  1.2488320 val acc  0.7360833, loss  0.9981124\n",
            "epoch 10, train acc  0.7258  0.6702 loss  1.0194177  1.2134859 val acc  0.7484982, loss  0.8948708\n",
            "epoch 11, train acc  0.7279  0.6728 loss  0.9943321  1.2085308 val acc  0.7308770, loss  1.0434633\n",
            "epoch 12, train acc  0.7285  0.6816 loss  0.9823744  1.1787646 val acc  0.7444934, loss  0.8906560\n",
            "epoch 13, train acc  0.7375  0.6716 loss  0.9396350  1.1553988 val acc  0.7340809, loss  0.9659434\n",
            "epoch 14, train acc  0.7383  0.6808 loss  0.9419021  1.1422160 val acc  0.7553064, loss  0.8446104\n",
            "epoch 15, train acc  0.7448  0.6914 loss  0.8994695  1.0808091 val acc  0.7448939, loss  0.9428513\n",
            "epoch 16, train acc  0.7495  0.6964 loss  0.8744551  1.0654272 val acc  0.7452943, loss  0.8942423\n",
            "epoch 17, train acc  0.7527  0.6930 loss  0.8391292  1.0325949 val acc  0.7420905, loss  0.9014050\n",
            "epoch 18, train acc  0.7564  0.7010 loss  0.8260080  1.0063789 val acc  0.7533039, loss  0.8604205\n",
            "epoch 19, train acc  0.7619  0.7070 loss  0.8093759  0.9867843 val acc  0.7456948, loss  0.8929151\n",
            "epoch 20, train acc  0.7630  0.7080 loss  0.7802743  0.9473705 val acc  0.7420905, loss  0.9070561\n",
            "epoch 21, train acc  0.7712  0.7162 loss  0.7567483  0.9360455 val acc  0.7517021, loss  0.8874700\n",
            "epoch 22, train acc  0.7755  0.7180 loss  0.7407793  0.9184760 val acc  0.7488987, loss  0.8371957\n",
            "epoch 23, train acc  0.7821  0.7306 loss  0.7128878  0.8827894 val acc  0.7521026, loss  0.8056272\n",
            "epoch 24, train acc  0.7780  0.7268 loss  0.7122523  0.8739522 val acc  0.7456948, loss  0.8648121\n",
            "epoch 25, train acc  0.7899  0.7412 loss  0.6890080  0.8483993 val acc  0.7476972, loss  0.8499989\n",
            "epoch 26, train acc  0.7871  0.7356 loss  0.6744945  0.8378794 val acc  0.7521026, loss  0.8519928\n",
            "epoch 27, train acc  0.7909  0.7480 loss  0.6560621  0.7980909 val acc  0.7476972, loss  0.8558398\n",
            "epoch 28, train acc  0.8001  0.7512 loss  0.6273900  0.7775016 val acc  0.7420905, loss  0.8920980\n",
            "epoch 29, train acc  0.7966  0.7532 loss  0.6261538  0.7667573 val acc  0.7480977, loss  0.8494700\n",
            "epoch 30, train acc  0.8068  0.7628 loss  0.6027388  0.7503392 val acc  0.7585102, loss  0.8029439\n",
            "epoch 31, train acc  0.8116  0.7666 loss  0.5824150  0.7182005 val acc  0.7521026, loss  0.8651096\n",
            "epoch 32, train acc  0.8097  0.7588 loss  0.5812123  0.7312999 val acc  0.7484982, loss  0.8426625\n",
            "epoch 33, train acc  0.8230  0.7746 loss  0.5565262  0.6907587 val acc  0.7501001, loss  0.8750541\n",
            "epoch 34, train acc  0.8214  0.7800 loss  0.5458395  0.6822297 val acc  0.7569083, loss  0.8236031\n",
            "epoch 35, train acc  0.8217  0.7738 loss  0.5379537  0.6687914 val acc  0.7597117, loss  0.8124101\n",
            "epoch 36, train acc  0.8270  0.7766 loss  0.5312998  0.6690485 val acc  0.7545054, loss  0.8432657\n",
            "epoch 37, train acc  0.8306  0.7856 loss  0.5153982  0.6531570 val acc  0.7488987, loss  0.8284406\n",
            "epoch 38, train acc  0.8282  0.7852 loss  0.5077114  0.6357840 val acc  0.7613136, loss  0.8433433\n",
            "epoch 39, train acc  0.8317  0.7904 loss  0.5098437  0.6363739 val acc  0.7460953, loss  0.9049869\n",
            "epoch 40, train acc  0.8330  0.7794 loss  0.4983001  0.6408328 val acc  0.7609131, loss  0.8567410\n",
            "epoch 41, train acc  0.8370  0.7882 loss  0.4919309  0.6232304 val acc  0.7545054, loss  0.8139434\n",
            "epoch 42, train acc  0.8502  0.8060 loss  0.4626398  0.5918638 val acc  0.7501001, loss  0.8508605\n",
            "epoch 43, train acc  0.8461  0.8014 loss  0.4617119  0.5909866 val acc  0.7577093, loss  0.8582312\n",
            "epoch 44, train acc  0.8452  0.8000 loss  0.4612865  0.5827660 val acc  0.7601122, loss  0.8230993\n",
            "epoch 45, train acc  0.8484  0.8050 loss  0.4546236  0.5726053 val acc  0.7521026, loss  0.8602601\n",
            "epoch 46, train acc  0.8529  0.7988 loss  0.4445738  0.5833864 val acc  0.7589107, loss  0.8425890\n",
            "epoch 47, train acc  0.8515  0.8082 loss  0.4387095  0.5648966 val acc  0.7573088, loss  0.8320555\n",
            "epoch 48, train acc  0.8589  0.8194 loss  0.4254403  0.5391709 val acc  0.7537044, loss  0.8577577\n",
            "epoch 49, train acc  0.8586  0.8182 loss  0.4264438  0.5472410 val acc  0.7653184, loss  0.8335716\n",
            "epoch 50, train acc  0.8633  0.8164 loss  0.4171891  0.5396335 val acc  0.7573088, loss  0.8601520\n",
            "epoch 51, train acc  0.8656  0.8218 loss  0.4122896  0.5369995 val acc  0.7585102, loss  0.8517006\n",
            "epoch 52, train acc  0.8614  0.8212 loss  0.4177160  0.5355689 val acc  0.7577093, loss  0.8224337\n",
            "epoch 53, train acc  0.8629  0.8204 loss  0.4080700  0.5316705 val acc  0.7585102, loss  0.8449160\n",
            "epoch 54, train acc  0.8676  0.8288 loss  0.3959539  0.5058691 val acc  0.7501001, loss  0.8914025\n",
            "epoch 55, train acc  0.8638  0.8218 loss  0.4044080  0.5248989 val acc  0.7537044, loss  0.8515581\n",
            "epoch 56, train acc  0.8639  0.8196 loss  0.4041513  0.5172365 val acc  0.7617140, loss  0.8527990\n",
            "epoch 57, train acc  0.8685  0.8264 loss  0.3973911  0.5076798 val acc  0.7509011, loss  0.8860689\n",
            "epoch 58, train acc  0.8680  0.8256 loss  0.3945429  0.5137151 val acc  0.7565078, loss  0.8572490\n",
            "epoch 59, train acc  0.8682  0.8288 loss  0.3913090  0.5033073 val acc  0.7645174, loss  0.8492029\n",
            "epoch 60, train acc  0.8745  0.8358 loss  0.3730618  0.4808385 val acc  0.7545054, loss  0.8674549\n",
            "epoch 61, train acc  0.8780  0.8364 loss  0.3698623  0.4817149 val acc  0.7637165, loss  0.8769323\n",
            "epoch 62, train acc  0.8696  0.8262 loss  0.3844736  0.5030086 val acc  0.7525030, loss  0.8651096\n",
            "epoch 63, train acc  0.8719  0.8248 loss  0.3766605  0.4872054 val acc  0.7601122, loss  0.8605395\n",
            "epoch 64, train acc  0.8726  0.8352 loss  0.3758270  0.4896033 val acc  0.7625150, loss  0.8489143\n",
            "epoch 65, train acc  0.8758  0.8342 loss  0.3703745  0.4868253 val acc  0.7581097, loss  0.8633999\n",
            "epoch 66, train acc  0.8755  0.8360 loss  0.3685216  0.4722267 val acc  0.7649179, loss  0.8537273\n",
            "epoch 67, train acc  0.8798  0.8446 loss  0.3548047  0.4538775 val acc  0.7565078, loss  0.8868313\n",
            "epoch 68, train acc  0.8801  0.8416 loss  0.3549425  0.4601225 val acc  0.7633160, loss  0.8686939\n",
            "epoch 69, train acc  0.8847  0.8442 loss  0.3435697  0.4483327 val acc  0.7693232, loss  0.8597677\n",
            "epoch 70, train acc  0.8791  0.8424 loss  0.3578919  0.4647845 val acc  0.7561073, loss  0.9040992\n",
            "epoch 71, train acc  0.8839  0.8444 loss  0.3448916  0.4556877 val acc  0.7601122, loss  0.8935562\n",
            "epoch 72, train acc  0.8836  0.8430 loss  0.3521469  0.4629736 val acc  0.7501001, loss  0.8881249\n",
            "epoch 73, train acc  0.8841  0.8442 loss  0.3450197  0.4472895 val acc  0.7605127, loss  0.8845648\n",
            "epoch 74, train acc  0.8884  0.8506 loss  0.3362892  0.4402272 val acc  0.7605127, loss  0.9076646\n",
            "epoch 75, train acc  0.8897  0.8494 loss  0.3257288  0.4262528 val acc  0.7641169, loss  0.9121708\n",
            "epoch 76, train acc  0.8931  0.8524 loss  0.3232821  0.4276171 val acc  0.7581097, loss  0.8625404\n",
            "epoch 77, train acc  0.8923  0.8494 loss  0.3273939  0.4503755 val acc  0.7669203, loss  0.8485994\n",
            "epoch 78, train acc  0.8905  0.8512 loss  0.3234896  0.4230190 val acc  0.7577093, loss  0.8712369\n",
            "epoch 79, train acc  0.8907  0.8566 loss  0.3244516  0.4266636 val acc  0.7625150, loss  0.8540011\n",
            "epoch 80, train acc  0.8932  0.8530 loss  0.3204102  0.4222888 val acc  0.7649179, loss  0.8768062\n",
            "epoch 81, train acc  0.8908  0.8394 loss  0.3303205  0.4511721 val acc  0.7673208, loss  0.8694053\n",
            "epoch 82, train acc  0.8906  0.8518 loss  0.3202164  0.4217673 val acc  0.7625150, loss  0.8648921\n",
            "epoch 83, train acc  0.8908  0.8486 loss  0.3243129  0.4257255 val acc  0.7585102, loss  0.9363276\n",
            "epoch 84, train acc  0.8958  0.8626 loss  0.3144179  0.4127393 val acc  0.7577093, loss  0.9084751\n",
            "epoch 85, train acc  0.8944  0.8554 loss  0.3131727  0.4151215 val acc  0.7689227, loss  0.8646405\n",
            "epoch 86, train acc  0.8908  0.8500 loss  0.3200550  0.4207599 val acc  0.7665198, loss  0.8616524\n",
            "epoch 87, train acc  0.8996  0.8644 loss  0.3047343  0.4001360 val acc  0.7625150, loss  0.8759709\n",
            "epoch 88, train acc  0.9017  0.8664 loss  0.3009303  0.4044345 val acc  0.7557068, loss  0.9256206\n",
            "epoch 89, train acc  0.8970  0.8572 loss  0.3114444  0.4122290 val acc  0.7633160, loss  0.8920029\n",
            "Best train\n",
            "{'worst_loss': 0.41222899317741396, 'avg_loss': 0.3114444109611213, 'worst_acc': 0.8571999722719192, 'avg_acc': 0.8969999761879444}\n",
            "Best val\n",
            "{'acc': 0.7693231701850891, 'loss': 0.8597677212953567}\n",
            "Test\n",
            "{'acc': 0.43412095308303833, 'loss': 2.4350413370132444}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_acc_values['oracle'] = test_res['acc']\n",
        "dict_loss_values['oracle'] = test_res['acc']"
      ],
      "metadata": {
        "id": "MOyTgY-5L_sP"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. RGM"
      ],
      "metadata": {
        "id": "FNh81sQ3N2a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_res(train_res, val_res, ep):\n",
        "    print((\"epoch {epoch}, train {acc} {train_acc:>7.4f} \"\n",
        "           \"{loss} {train_loss:>10.7f}, {regret} {train_regret:>10.7f} \"\n",
        "           \"val {acc} {val_acc:>10.7f}, {loss} {val_loss:>10.7f}\").format(\n",
        "               epoch=ep,\n",
        "               acc=colored(\"acc\", \"blue\"),\n",
        "               loss=colored(\"loss\", \"yellow\"),\n",
        "               regret=colored(\"regret\", \"red\"),\n",
        "               train_acc=train_res[\"acc\"],\n",
        "               train_loss=train_res[\"loss\"],\n",
        "               train_regret=train_res[\"regret\"],\n",
        "               val_acc=val_res[\"acc\"],\n",
        "               val_loss=val_res[\"loss\"]), flush=True)"
      ],
      "metadata": {
        "id": "i27diNlvRLgq"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_worst_acc(true, pred, idx, loss, att_idx_dict):\n",
        "    acc_list = (true == pred).float().tolist()\n",
        "    idx = torch.cat(idx).tolist()\n",
        "    idx_origin2new = dict(zip(idx, range(len(idx))))\n",
        "\n",
        "    verbose = True\n",
        "    if len(att_idx_dict) == 1:  # validation\n",
        "        verbose = False\n",
        "\n",
        "    worst_acc_list = []\n",
        "    avg_acc_list = []\n",
        "\n",
        "    for att, data_dict in att_idx_dict.items():\n",
        "        if verbose:\n",
        "            print('{:>20}'.format(att), end=' ')\n",
        "\n",
        "        worst_acc = 1\n",
        "        avg_acc = []\n",
        "        for k, v in data_dict.items():\n",
        "            # value to index mapping\n",
        "            acc = []\n",
        "            for origin in v:\n",
        "                acc.append(acc_list[idx_origin2new[origin]])\n",
        "\n",
        "            if len(acc) > 0:\n",
        "                cur_acc = np.mean(acc)\n",
        "\n",
        "                if verbose:\n",
        "                    print(k, ' {:>8}'.format(len(v)),\n",
        "                          ' {:>7.4f}'.format(cur_acc), end=', ')\n",
        "                if cur_acc < worst_acc:\n",
        "                    worst_acc = cur_acc\n",
        "                avg_acc.append(cur_acc)\n",
        "\n",
        "        if verbose:\n",
        "            print(' worst: {:>7.4f}'.format(worst_acc))\n",
        "\n",
        "        avg_acc = np.mean(avg_acc)\n",
        "        worst_acc_list.append(worst_acc)\n",
        "        avg_acc_list.append(avg_acc)\n",
        "\n",
        "    return {\n",
        "        'acc': np.mean(worst_acc_list),\n",
        "        'avg_acc': np.mean(avg_acc_list),\n",
        "        'loss': loss,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "rYpiLhPARLgq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(train_loaders, model, opt_all, opt_0, opt_1, ep):\n",
        "    stats = {}\n",
        "    for k in ['acc', 'loss', 'regret', 'loss_train']:\n",
        "        stats[k] = []\n",
        "\n",
        "    step = ep * num_batches\n",
        "    for batch_0, batch_1 in zip(train_loaders[0], train_loaders[1]):\n",
        "        # work on each batch\n",
        "        model['ebd'].train()\n",
        "        model['clf_all'].train()\n",
        "        model['clf_0'].train()\n",
        "        model['clf_1'].train()\n",
        "\n",
        "        batch_0 = to_cuda(squeeze_batch(batch_0))\n",
        "        batch_1 = to_cuda(squeeze_batch(batch_1))\n",
        "\n",
        "        x_0 = model['ebd'](batch_0['X'])\n",
        "        y_0 = batch_0['Y']\n",
        "        x_1 = model['ebd'](batch_1['X'])\n",
        "        y_1 = batch_1['Y']\n",
        "\n",
        "        # train clf_0 on x_0\n",
        "        _, loss_0_0 = model['clf_0'](x_0.detach(), y_0)\n",
        "        opt_0.zero_grad()\n",
        "        loss_0_0.backward()\n",
        "        opt_0.step()\n",
        "\n",
        "        # train clf_1 on x_1\n",
        "        _, loss_1_1 = model['clf_1'](x_1.detach(), y_1)\n",
        "        opt_1.zero_grad()\n",
        "        loss_1_1.backward()\n",
        "        opt_1.step()\n",
        "\n",
        "        # train clf_all on both, backprop to representation\n",
        "        x = torch.cat([x_0, x_1], dim=0)\n",
        "        y = torch.cat([y_0, y_1], dim=0)\n",
        "        acc, loss_ce = model['clf_all'](x, y)\n",
        "\n",
        "        # randomly sample a group, evaluate the validation loss\n",
        "        # do not detach feature representation at this time\n",
        "        if random.random() > 0.5:\n",
        "            # choose env 1\n",
        "            # apply clf 1 on env 1\n",
        "            _, loss_1_1 = model['clf_1'](x_1, y_1)\n",
        "\n",
        "            # apply clf 0 on env 1\n",
        "            _, loss_0_1 = model['clf_0'](x_1, y_1)\n",
        "\n",
        "            regret = loss_0_1 - loss_1_1\n",
        "        else:\n",
        "            # chosse env 0\n",
        "            # apply clf 1 on env 0\n",
        "            _, loss_1_0 = model['clf_1'](x_0, y_0)\n",
        "\n",
        "            # apply clf 0 on env 0\n",
        "            _, loss_0_0 = model['clf_0'](x_0, y_0)\n",
        "\n",
        "            regret = loss_1_0 - loss_0_0\n",
        "\n",
        "        weight = l_regret if step > anneal_iters else 1.0\n",
        "        loss = loss_ce + weight * regret\n",
        "        step += 1\n",
        "\n",
        "        if weight > 1.0:\n",
        "            loss /= weight\n",
        "\n",
        "        opt_all.zero_grad()\n",
        "        opt_0.zero_grad()\n",
        "        opt_1.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_all.step()\n",
        "\n",
        "        stats['acc'].append(acc)\n",
        "        stats['loss'].append(loss.item())\n",
        "        stats['loss_train'].append(loss_ce.item())\n",
        "        stats['regret'].append(regret.item())\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        stats[k] = float(np.mean(np.array(v)))\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def test_loop(test_loader, model, ep, att_idx_dict=None):\n",
        "    loss_list = []\n",
        "    true, pred = [], []\n",
        "    if att_idx_dict is not None:\n",
        "        idx = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        # work on each batch\n",
        "        model['ebd'].eval()\n",
        "        model['clf_all'].eval()\n",
        "\n",
        "        batch = to_cuda(squeeze_batch(batch))\n",
        "\n",
        "        x = model['ebd'](batch['X'])\n",
        "        y = batch['Y']\n",
        "\n",
        "        y_hat, loss = model['clf_all'](x, y, return_pred=True)\n",
        "\n",
        "        true.append(y)\n",
        "        pred.append(y_hat)\n",
        "\n",
        "        if att_idx_dict is not None:\n",
        "            idx.append(batch['idx'])\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    true = torch.cat(true)\n",
        "    pred = torch.cat(pred)\n",
        "\n",
        "    acc = torch.mean((true == pred).float()).item()\n",
        "    loss = np.mean(np.array(loss_list))\n",
        "\n",
        "    if att_idx_dict is not None:\n",
        "        return get_worst_acc(true, pred, idx, loss, att_idx_dict)\n",
        "\n",
        "    return {\n",
        "        'acc': acc,\n",
        "        'loss': loss,\n",
        "    }\n",
        "\n",
        "\n",
        "def rgm(train_data, test_data, model, opt):\n",
        "    train_loaders = []\n",
        "    for i in range(2):\n",
        "        train_loaders.append(DataLoader(\n",
        "            train_data,\n",
        "            sampler=EnvSampler(num_batches, batch_size, i,\n",
        "                               train_data.envs[i]['idx_list']),\n",
        "        num_workers=7))\n",
        "\n",
        "    test_loaders = []\n",
        "    for i in range(4):\n",
        "        test_loaders.append(DataLoader(\n",
        "            test_data,\n",
        "            sampler=EnvSampler(-1, batch_size, i,\n",
        "                               test_data.envs[i]['idx_list']),\n",
        "        num_workers=7))\n",
        "\n",
        "    # start training\n",
        "    best_acc = -1\n",
        "    best_val_res = None\n",
        "    best_model = {}\n",
        "    cycle = 0\n",
        "    opt_all = opt[0]\n",
        "    opt_0 = opt[1]\n",
        "    opt_1 = opt[2]\n",
        "    for ep in range(num_epochs):\n",
        "        train_res = train_loop(train_loaders, model, opt_all, opt_0, opt_1, ep)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # validation\n",
        "            val_res = test_loop(test_loaders[2], model, ep,\n",
        "                                test_data.val_att_idx_dict)\n",
        "\n",
        "        print_res(train_res, val_res, ep)\n",
        "\n",
        "        if min(train_res['acc'], val_res['acc']) > best_acc:\n",
        "            best_acc = min(train_res['acc'], val_res['acc'])\n",
        "            best_val_res = val_res\n",
        "            best_train_res = train_res\n",
        "            cycle = 0\n",
        "            # save best ebd\n",
        "            for k in 'ebd', 'clf_all':\n",
        "                best_model[k] = copy.deepcopy(model[k].state_dict())\n",
        "        else:\n",
        "            cycle += 1\n",
        "\n",
        "        if cycle == patience:\n",
        "            break\n",
        "\n",
        "    # load best model\n",
        "    for k in 'ebd', 'clf_all':\n",
        "        model[k].load_state_dict(best_model[k])\n",
        "\n",
        "    # get the results\n",
        "    test_res = test_loop(test_loaders[3], model, ep,\n",
        "                         test_data.test_att_idx_dict)\n",
        "    print('Best train')\n",
        "    print(train_res)\n",
        "    print('Best val')\n",
        "    val_res = best_val_res\n",
        "    print(val_res)\n",
        "    print('Test')\n",
        "    print(test_res)\n",
        "\n",
        "    return train_res, val_res, test_res\n"
      ],
      "metadata": {
        "id": "IJT0U4pmQUHk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model(\"MNIST\", \"rgm\")"
      ],
      "metadata": {
        "id": "BbkgGtq2hXr8"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_res, val_res, test_res = rgm(train_data, test_data, model, opt)"
      ],
      "metadata": {
        "id": "w5EeQgjEhXr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62560e3-fca4-4c11-968d-7b8203a5811b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train acc  0.7954 loss  0.9070281, regret  0.0441490 val acc  0.8021626, loss  0.7683486\n",
            "epoch 1, train acc  0.8359 loss  0.7148660, regret  0.0543811 val acc  0.7965559, loss  0.7461339\n",
            "epoch 2, train acc  0.8316 loss  0.6828225, regret  0.0390593 val acc  0.8025631, loss  0.7359535\n",
            "epoch 3, train acc  0.8360 loss  0.6731453, regret  0.0434204 val acc  0.7973568, loss  0.7375603\n",
            "epoch 4, train acc  0.8298 loss  0.6786485, regret  0.0424215 val acc  0.8005607, loss  0.7286372\n",
            "epoch 5, train acc  0.8364 loss  0.6389910, regret  0.0371922 val acc  0.7929515, loss  0.7443935\n",
            "epoch 6, train acc  0.8275 loss  0.6621757, regret  0.0399377 val acc  0.7981578, loss  0.7140659\n",
            "epoch 7, train acc  0.8297 loss  0.6610126, regret  0.0358753 val acc  0.7985583, loss  0.7407011\n",
            "epoch 8, train acc  0.8309 loss  0.6283940, regret  0.0301050 val acc  0.7941530, loss  0.7237664\n",
            "epoch 9, train acc  0.8303 loss  0.6451355, regret  0.0441423 val acc  0.8009611, loss  0.7005494\n",
            "epoch 10, train acc  0.8350 loss  0.6297227, regret  0.0308678 val acc  0.7957549, loss  0.6908236\n",
            "epoch 11, train acc  0.8307 loss  0.6326723, regret  0.0308901 val acc  0.7949539, loss  0.6910721\n",
            "epoch 12, train acc  0.8309 loss  0.6289424, regret  0.0365374 val acc  0.8033640, loss  0.6895566\n",
            "epoch 13, train acc  0.8252 loss  0.6387450, regret  0.0341314 val acc  0.7953544, loss  0.7016276\n",
            "epoch 14, train acc  0.8300 loss  0.6212626, regret  0.0381794 val acc  0.7961554, loss  0.6710384\n",
            "epoch 15, train acc  0.8406 loss  0.5806381, regret  0.0192139 val acc  0.7933520, loss  0.7014088\n",
            "epoch 16, train acc  0.8394 loss  0.5914797, regret  0.0283492 val acc  0.7977573, loss  0.6916000\n",
            "epoch 17, train acc  0.8364 loss  0.5805971, regret  0.0200547 val acc  0.8025631, loss  0.6758979\n",
            "epoch 18, train acc  0.8367 loss  0.5885123, regret  0.0317906 val acc  0.7841410, loss  0.6921203\n",
            "epoch 19, train acc  0.8379 loss  0.5787548, regret  0.0325696 val acc  0.7989588, loss  0.6496794\n",
            "epoch 20, train acc  0.8334 loss  0.5964581, regret  0.0380929 val acc  0.7929515, loss  0.6900924\n",
            "epoch 21, train acc  0.8360 loss  0.5954941, regret  0.0261698 val acc  0.7965559, loss  0.6677768\n",
            "epoch 22, train acc  0.8408 loss  0.5638047, regret  0.0291108 val acc  0.7981578, loss  0.7003906\n",
            "epoch 23, train acc  0.8420 loss  0.5456565, regret  0.0190115 val acc  0.7953544, loss  0.6750852\n",
            "epoch 24, train acc  0.8349 loss  0.5688180, regret  0.0273314 val acc  0.8017621, loss  0.6613241\n",
            "epoch 25, train acc  0.8363 loss  0.5727478, regret  0.0304442 val acc  0.7889467, loss  0.6659356\n",
            "epoch 26, train acc  0.8421 loss  0.5386935, regret  0.0200394 val acc  0.7925511, loss  0.6542750\n",
            "epoch 27, train acc  0.8386 loss  0.5717464, regret  0.0333700 val acc  0.7929515, loss  0.6412068\n",
            "epoch 28, train acc  0.8422 loss  0.5483750, regret  0.0264361 val acc  0.7969564, loss  0.6621109\n",
            "epoch 29, train acc  0.8363 loss  0.5589416, regret  0.0349736 val acc  0.7905487, loss  0.6705471\n",
            "epoch 30, train acc  0.8416 loss  0.5447964, regret  0.0231396 val acc  0.7993593, loss  0.6517660\n",
            "epoch 31, train acc  0.8378 loss  0.5509140, regret  0.0268848 val acc  0.7937525, loss  0.6671297\n",
            "epoch 32, train acc  0.8395 loss  0.5460337, regret  0.0268133 val acc  0.7949539, loss  0.6824209\n",
            "Best train\n",
            "{'acc': 0.8394999754428863, 'loss': 0.5460336795449257, 'regret': 0.026813302636146546, 'loss_train': 0.5192203772068024}\n",
            "Best val\n",
            "{'acc': 0.8033640384674072, 'loss': 0.6895566344261169}\n",
            "Test\n",
            "{'acc': 0.20104125142097473, 'loss': 2.353083851337433}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_acc_values['rgm'] = test_res['acc']\n",
        "dict_loss_values['rgm'] = test_res['acc']"
      ],
      "metadata": {
        "id": "IMz5yf3nSH5b"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. OURS - PI\n",
        "\n",
        "The following code blocks contain the algorithm which was created by the authors, which is Predict Interpolate."
      ],
      "metadata": {
        "id": "pobeb0ptS5Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dro_loop(train_loaders, model, opt, ep):\n",
        "    stats = {}\n",
        "    for k in ['worst_loss', 'avg_loss', 'worst_acc', 'avg_acc']:\n",
        "        stats[k] = []\n",
        "\n",
        "    step = 0\n",
        "    for batches in zip(*train_loaders):\n",
        "        # work on each batch\n",
        "        model['ebd'].train()\n",
        "        model['clf_all'].train()\n",
        "\n",
        "        x, y = [], []\n",
        "\n",
        "        for batch in batches:\n",
        "            batch = to_cuda(squeeze_batch(batch))\n",
        "            x.append(batch['X'])\n",
        "            y.append(batch['Y'])\n",
        "\n",
        "        if 'beer' in dataset or 'pubmed' in dataset:\n",
        "            # text models have varying length between batches\n",
        "            pred = []\n",
        "            for cur_x in x:\n",
        "                pred.append(model['clf_all'](model['ebd'](cur_x)))\n",
        "            pred = torch.cat(pred, dim=0)\n",
        "        else:\n",
        "            pred = model['clf_all'](model['ebd'](torch.cat(x, dim=0)))\n",
        "\n",
        "        cur_idx = 0\n",
        "\n",
        "        avg_loss = 0\n",
        "        avg_acc = 0\n",
        "        worst_loss = 0\n",
        "        worst_acc = 0\n",
        "\n",
        "        for cur_true in y:\n",
        "            cur_pred = pred[cur_idx:cur_idx+len(cur_true)]\n",
        "            cur_idx += len(cur_true)\n",
        "\n",
        "            loss = F.cross_entropy(cur_pred, cur_true)\n",
        "            acc = torch.mean((torch.argmax(cur_pred, dim=1) == cur_true).float()).item()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "            avg_acc += acc\n",
        "\n",
        "            if loss.item() > worst_loss:\n",
        "                worst_loss = loss\n",
        "                worst_acc = acc\n",
        "\n",
        "        opt.zero_grad()\n",
        "        worst_loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        avg_loss /= len(y)\n",
        "        avg_acc /= len(y)\n",
        "\n",
        "        stats['avg_acc'].append(avg_acc)\n",
        "        stats['avg_loss'].append(avg_loss)\n",
        "        stats['worst_acc'].append(worst_acc)\n",
        "        stats['worst_loss'].append(worst_loss.item())\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        stats[k] = float(np.mean(np.array(v)))\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "iVLOSTMME2h5"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(train_loader, model, opt, ep):\n",
        "    stats = {}\n",
        "    for k in ['acc', 'loss']:\n",
        "        stats[k] = []\n",
        "\n",
        "    step = 0\n",
        "    for batch in train_loader:\n",
        "        # work on each batch\n",
        "        model['ebd'].train()\n",
        "        model['clf_all'].train()\n",
        "\n",
        "        batch = to_cuda(squeeze_batch(batch))\n",
        "\n",
        "        x = model['ebd'](batch['X'])\n",
        "        y = batch['Y']\n",
        "\n",
        "        acc, loss = model['clf_all'](x, y, return_pred=False,\n",
        "                                     grad_penalty=False)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        stats['acc'].append(acc)\n",
        "        stats['loss'].append(loss.item())\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        stats[k] = float(np.mean(np.array(v)))\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "cunCZ7fYSYTj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(test_loader, model, ep, return_idx=False, att_idx_dict=None):\n",
        "    loss_list = []\n",
        "    true, pred, cor = [], [], []\n",
        "    if (att_idx_dict is not None) or return_idx:\n",
        "        idx = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        # work on each batch\n",
        "        model['ebd'].eval()\n",
        "        model['clf_all'].eval()\n",
        "\n",
        "        batch = to_cuda(squeeze_batch(batch))\n",
        "\n",
        "        x = model['ebd'](batch['X'])\n",
        "        y = batch['Y']\n",
        "        c = batch['C']\n",
        "\n",
        "        y_hat, loss = model['clf_all'](x, y, return_pred=True)\n",
        "\n",
        "        true.append(y)\n",
        "        pred.append(y_hat)\n",
        "        cor.append(c)\n",
        "\n",
        "        if (att_idx_dict is not None) or return_idx:\n",
        "            idx.append(batch['idx'])\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    true = torch.cat(true)\n",
        "    pred = torch.cat(pred)\n",
        "\n",
        "    acc = torch.mean((true == pred).float()).item()\n",
        "    loss = np.mean(np.array(loss_list))\n",
        "\n",
        "    if return_idx:\n",
        "        cor = torch.cat(cor).tolist()\n",
        "        true = true.tolist()\n",
        "        pred = pred.tolist()\n",
        "        idx = torch.cat(idx).tolist()\n",
        "\n",
        "        # split correct and wrong idx\n",
        "        correct_idx, wrong_idx = [], []\n",
        "\n",
        "        # compute correlation between cor and y for analysis\n",
        "        correct_cor, wrong_cor = [], []\n",
        "        correct_y, wrong_y = [], []\n",
        "\n",
        "        for i, y, y_hat, c in zip(idx, true, pred, cor):\n",
        "            if y == y_hat:\n",
        "                correct_idx.append(i)\n",
        "                correct_cor.append(c)\n",
        "                correct_y.append(y)\n",
        "            else:\n",
        "                wrong_idx.append(i)\n",
        "                wrong_cor.append(c)\n",
        "                wrong_y.append(y)\n",
        "\n",
        "        return {\n",
        "            'acc': acc,\n",
        "            'loss': loss,\n",
        "            'correct_idx': correct_idx,\n",
        "            'correct_cor': correct_cor,\n",
        "            'correct_y': correct_y,\n",
        "            'wrong_idx': wrong_idx,\n",
        "            'wrong_cor': wrong_cor,\n",
        "            'wrong_y': wrong_y,\n",
        "        }\n",
        "\n",
        "    if att_idx_dict is not None:\n",
        "        return get_worst_acc(true, pred, idx, loss, att_idx_dict)\n",
        "\n",
        "    return {\n",
        "        'acc': acc,\n",
        "        'loss': loss,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "xHseTA_qSgwm"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_res(train_res, val_res, ep):\n",
        "    print((\"epoch {epoch}, train {acc} {train_acc:>7.4f} {train_worst_acc:>7.4f} \"\n",
        "           \"{loss} {train_loss:>10.7f} {train_worst_loss:>10.7f} \"\n",
        "           \"val {acc} {val_acc:>10.7f}, {loss} {val_loss:>10.7f}\").format(\n",
        "               epoch=ep,\n",
        "               acc=colored(\"acc\", \"blue\"),\n",
        "               loss=colored(\"loss\", \"yellow\"),\n",
        "               regret=colored(\"regret\", \"red\"),\n",
        "               train_acc=train_res[\"avg_acc\"],\n",
        "               train_worst_acc=train_res[\"worst_acc\"],\n",
        "               train_loss=train_res[\"avg_loss\"],\n",
        "               train_worst_loss=train_res[\"worst_loss\"],\n",
        "               val_acc=val_res[\"acc\"],\n",
        "               val_loss=val_res[\"loss\"]), flush=True)"
      ],
      "metadata": {
        "id": "M-K1k83gSqK3"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_group_stats(pretrain_res, train_data=None):\n",
        "    if train_data is None:\n",
        "        for i in range(len(pretrain_res)):\n",
        "            print('{}on{}_correct '.format(1-i, i), end='')\n",
        "            print('len: {:7d}, correlation: {:>7.4f}'.format(\n",
        "                len(pretrain_res[i]['correct_idx']),\n",
        "                np.corrcoef(pretrain_res[i]['correct_cor'],\n",
        "                            pretrain_res[i]['correct_y'])[0,1]))\n",
        "\n",
        "            print('{}on{}_wrong   '.format(1-i, i), end='')\n",
        "            print('len: {:7d}, correlation: {:>7.4f}'.format(\n",
        "                len(pretrain_res[i]['wrong_idx']),\n",
        "                np.corrcoef(pretrain_res[i]['wrong_cor'],\n",
        "                            pretrain_res[i]['wrong_y'])[0,1]))\n",
        "    else:\n",
        "        # print per attribute correlation for each group\n",
        "        results = []\n",
        "        for i in range(len(pretrain_res)):\n",
        "            # for env i\n",
        "            attr_matrix = train_data.get_all_att(i)\n",
        "            y_correct = attr_matrix[pretrain_res[i]['correct_idx'],\n",
        "                                    train_data.label_idx]\n",
        "            y_wrong = attr_matrix[pretrain_res[i]['wrong_idx'],\n",
        "                                  train_data.label_idx]\n",
        "            y_all = attr_matrix[train_data.envs[i]['idx_list'],\n",
        "                                train_data.label_idx]\n",
        "\n",
        "            # print(len(train_data.envs[i]['idx_list']))\n",
        "            # print(len(pretrain_res[i]['correct_idx']))\n",
        "            # print(len(pretrain_res[i]['wrong_idx']))\n",
        "\n",
        "            for att_idx, attr in enumerate(torch.transpose(attr_matrix, 0, 1)):\n",
        "                c_correct = attr[pretrain_res[i]['correct_idx']]\n",
        "                c_wrong = attr[pretrain_res[i]['wrong_idx']]\n",
        "                c_all = attr[train_data.envs[i]['idx_list']]\n",
        "                rho_all = np.corrcoef(y_all.tolist(), c_all.tolist())[0, 1]\n",
        "                rho_correct = np.corrcoef(y_correct.tolist(), c_correct.tolist())[0, 1]\n",
        "                rho_wrong = np.corrcoef(y_wrong.tolist(), c_wrong.tolist())[0, 1]\n",
        "                if i == 0:\n",
        "                    results.append([rho_all, rho_correct, rho_wrong])\n",
        "                else:\n",
        "                    results[att_idx].append(rho_all)\n",
        "                    results[att_idx].append(rho_correct)\n",
        "                    results[att_idx].append(rho_wrong)\n",
        "\n",
        "        print('0_all, 1_0_correct, 1_0_wrong, 1_all,  0_1_correct, 0_1_wrong')\n",
        "        for i, rhos in enumerate(results):\n",
        "            print(train_data.get_att_names(i), end=', ')\n",
        "            for r in rhos:\n",
        "                print('{:>7.4f}'.format(r), end=', ')\n",
        "            print()\n"
      ],
      "metadata": {
        "id": "94pMMz7vSu2D"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_pretrain_res(train_res, test_res, ep, i):\n",
        "    print((\"petrain {i}, epoch {epoch}, train {acc} {train_acc:>7.4f} \"\n",
        "           \"{loss} {train_loss:>7.4f}, \"\n",
        "           \"val {acc} {test_acc:>7.4f}, {loss} {test_loss:>7.4f} \").format(\n",
        "               epoch=ep,\n",
        "               i = i,\n",
        "               acc=colored(\"acc\", \"blue\"),\n",
        "               loss=colored(\"loss\", \"yellow\"),\n",
        "               ebd=colored(\"ebd\", \"red\"),\n",
        "               train_acc=train_res[\"acc\"],\n",
        "               train_loss=train_res[\"loss\"],\n",
        "               test_acc=test_res[\"acc\"],\n",
        "               test_loss=test_res[\"loss\"]), flush=True)\n"
      ],
      "metadata": {
        "id": "BTG7wlPjSz1c"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ours(train_data, test_data, model, opt):\n",
        "    train_loaders = []\n",
        "    for i in range(2):\n",
        "        train_loaders.append(DataLoader(\n",
        "            train_data,\n",
        "            sampler=EnvSampler(num_batches, batch_size, i,\n",
        "                               train_data.envs[i]['idx_list']),\n",
        "        num_workers=7))\n",
        "\n",
        "    test_loaders = []\n",
        "    for i in range(4):\n",
        "        test_loaders.append(DataLoader(\n",
        "            test_data,\n",
        "            sampler=EnvSampler(-1, batch_size, i,\n",
        "                               test_data.envs[i]['idx_list']),\n",
        "        num_workers=7))\n",
        "\n",
        "    # training the environment-specific classifier\n",
        "    models = []\n",
        "    for i in range(2):\n",
        "        if hasattr(train_data, 'vocab'):\n",
        "            cur_model, cur_opt = get_model(\"MNIST\", \"ours\")\n",
        "        else:\n",
        "            cur_model, cur_opt = get_model(\"MNIST\", \"ours\")\n",
        "\n",
        "        print(\"{}, Start training classifier on train env {}\".format(\n",
        "            datetime.datetime.now().strftime('%m-%d-%Y %H:%M%p'), i),\n",
        "              flush=True)\n",
        "\n",
        "        best_acc = -1\n",
        "        best_model = {}\n",
        "        cycle = 0\n",
        "\n",
        "        # start training the env specific model\n",
        "        for ep in range(num_epochs):\n",
        "            train_res = train_loop(train_loaders[i], cur_model, cur_opt, ep)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # evaluate on the other training environment\n",
        "                val_res = test_loop(train_loaders[1-i], cur_model, ep)\n",
        "\n",
        "            print_pretrain_res(train_res, val_res, ep, i)\n",
        "\n",
        "            # writer.add_scalar('pretrain_{}/train_env'.format(i), train_res['acc'], ep)\n",
        "            # writer.add_scalar('pretrain_{}/test_env'.format(i), val_res['acc'], ep)\n",
        "            # writer.add_scalar('pretrain_{}/train_env_ce'.format(i), train_res['loss'], ep)\n",
        "\n",
        "            # if min(train_res['acc'], test_res['acc']) > best_acc:\n",
        "            if val_res['acc'] > best_acc:\n",
        "                best_acc = val_res['acc']\n",
        "                cycle = 0\n",
        "                # save best ebd\n",
        "                for k in 'ebd', 'clf_all':\n",
        "                    best_model[k] = copy.deepcopy(cur_model[k].state_dict())\n",
        "            else:\n",
        "                cycle += 1\n",
        "\n",
        "            if cycle == patience:\n",
        "                break\n",
        "\n",
        "        # load best model\n",
        "        for k in 'ebd', 'clf_all':\n",
        "            cur_model[k].load_state_dict(best_model[k])\n",
        "\n",
        "        models.append(cur_model)\n",
        "\n",
        "    # load training data in test mode\n",
        "    test_train_loaders = []\n",
        "    for i in range(2):\n",
        "        test_train_loaders.append(DataLoader(train_data,\n",
        "            sampler=EnvSampler(-1, batch_size, i,\n",
        "                               train_data.envs[i]['idx_list']), num_workers=7))\n",
        "\n",
        "    # split the dataset based on the model predictions\n",
        "    pretrain_res = []\n",
        "    pretrain_res.append(test_loop(test_train_loaders[0], models[1], ep, True))\n",
        "    pretrain_res.append(test_loop(test_train_loaders[1], models[0], ep, True))\n",
        "\n",
        "    if dataset in ['pubmed', 'celeba']:\n",
        "        print_group_stats(pretrain_res, train_data)\n",
        "    else:\n",
        "        print_group_stats(pretrain_res)\n",
        "\n",
        "    # train a new unbiased model through dro\n",
        "    train_loaders = []\n",
        "    print('\\n######################\\nCreate New Groups')\n",
        "    for i in range(len(pretrain_res)):\n",
        "        train_loaders.append(DataLoader(\n",
        "            train_data, sampler=EnvSampler(num_batches, batch_size, i,\n",
        "                                           pretrain_res[i]['correct_idx']),\n",
        "            num_workers=10))\n",
        "\n",
        "        train_loaders.append(DataLoader(\n",
        "            train_data, sampler=EnvSampler(num_batches, batch_size, i,\n",
        "                                           pretrain_res[i]['wrong_idx']),\n",
        "        num_workers=10))\n",
        "\n",
        "    # start training\n",
        "    best_acc = -1\n",
        "    best_val_res = None\n",
        "    best_model = {}\n",
        "    cycle = 0\n",
        "    for ep in range(num_epochs):\n",
        "        train_res = train_dro_loop(train_loaders, model, opt, ep)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # validation\n",
        "            val_res = test_loop(test_loaders[2], model, ep,\n",
        "                                att_idx_dict=test_data.val_att_idx_dict)\n",
        "\n",
        "        print_res(train_res, val_res, ep)\n",
        "\n",
        "        if min(train_res['worst_acc'], val_res['acc']) > best_acc:\n",
        "            best_acc = min(train_res['worst_acc'], val_res['acc'])\n",
        "            best_val_res = val_res\n",
        "            best_train_res = train_res\n",
        "            cycle = 0\n",
        "            # save best ebd\n",
        "            for k in 'ebd', 'clf_all':\n",
        "                best_model[k] = copy.deepcopy(model[k].state_dict())\n",
        "        else:\n",
        "            cycle += 1\n",
        "\n",
        "        if cycle == patience:\n",
        "            break\n",
        "\n",
        "    # load best model\n",
        "    for k in 'ebd', 'clf_all':\n",
        "        model[k].load_state_dict(best_model[k])\n",
        "\n",
        "    # get the results\n",
        "    test_res = test_loop(test_loaders[3], model, ep,\n",
        "                         att_idx_dict=test_data.test_att_idx_dict)\n",
        "    print('Best train')\n",
        "    print(train_res)\n",
        "    print('Best val')\n",
        "    val_res = best_val_res\n",
        "    print(val_res)\n",
        "    print('Test')\n",
        "    print(test_res)\n",
        "\n",
        "    return train_res, val_res, test_res\n"
      ],
      "metadata": {
        "id": "F3v1tMbyS5vw"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model(\"MNIST\", \"ours\")"
      ],
      "metadata": {
        "id": "rrZ648OsY_1N"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_res, val_res, test_res = ours(train_data, test_data, model, opt)"
      ],
      "metadata": {
        "id": "XRZ1dKjyVaL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7798eddb-656b-47ca-f338-c2d7ba3a2196"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04-25-2023 16:07PM, Start training classifier on train env 0\n",
            "petrain 0, epoch 0, train acc  0.8444 loss  0.7353, val acc  0.7954, loss  0.9164 \n",
            "petrain 0, epoch 1, train acc  0.8890 loss  0.4927, val acc  0.7894, loss  0.8155 \n",
            "petrain 0, epoch 2, train acc  0.8932 loss  0.4531, val acc  0.8014, loss  0.8445 \n",
            "petrain 0, epoch 3, train acc  0.8884 loss  0.4846, val acc  0.7946, loss  0.8844 \n",
            "petrain 0, epoch 4, train acc  0.8870 loss  0.4683, val acc  0.8050, loss  0.8349 \n",
            "petrain 0, epoch 5, train acc  0.8882 loss  0.4575, val acc  0.7910, loss  0.7941 \n",
            "petrain 0, epoch 6, train acc  0.8968 loss  0.4430, val acc  0.8016, loss  0.8614 \n",
            "petrain 0, epoch 7, train acc  0.8920 loss  0.4471, val acc  0.7964, loss  0.7869 \n",
            "petrain 0, epoch 8, train acc  0.8938 loss  0.4283, val acc  0.7864, loss  0.8470 \n",
            "petrain 0, epoch 9, train acc  0.8882 loss  0.4476, val acc  0.7948, loss  0.7989 \n",
            "petrain 0, epoch 10, train acc  0.8876 loss  0.4480, val acc  0.7994, loss  0.7993 \n",
            "petrain 0, epoch 11, train acc  0.8962 loss  0.4353, val acc  0.7992, loss  0.7924 \n",
            "petrain 0, epoch 12, train acc  0.8950 loss  0.4083, val acc  0.7956, loss  0.8130 \n",
            "petrain 0, epoch 13, train acc  0.8902 loss  0.4400, val acc  0.7930, loss  0.7623 \n",
            "petrain 0, epoch 14, train acc  0.8862 loss  0.4485, val acc  0.7970, loss  0.7820 \n",
            "petrain 0, epoch 15, train acc  0.8920 loss  0.4197, val acc  0.7968, loss  0.8225 \n",
            "petrain 0, epoch 16, train acc  0.8832 loss  0.4388, val acc  0.8000, loss  0.8019 \n",
            "petrain 0, epoch 17, train acc  0.8864 loss  0.4274, val acc  0.7836, loss  0.8262 \n",
            "petrain 0, epoch 18, train acc  0.8930 loss  0.4332, val acc  0.8022, loss  0.6863 \n",
            "petrain 0, epoch 19, train acc  0.8950 loss  0.4068, val acc  0.7928, loss  0.7668 \n",
            "petrain 0, epoch 20, train acc  0.8858 loss  0.4089, val acc  0.8014, loss  0.8092 \n",
            "petrain 0, epoch 21, train acc  0.8928 loss  0.4147, val acc  0.7944, loss  0.8497 \n",
            "petrain 0, epoch 22, train acc  0.8902 loss  0.4234, val acc  0.8014, loss  0.7636 \n",
            "petrain 0, epoch 23, train acc  0.8920 loss  0.4195, val acc  0.7966, loss  0.7587 \n",
            "petrain 0, epoch 24, train acc  0.8884 loss  0.4138, val acc  0.8020, loss  0.7542 \n",
            "04-25-2023 16:09PM, Start training classifier on train env 1\n",
            "petrain 1, epoch 0, train acc  0.7394 loss  1.0235, val acc  0.8686, loss  0.5422 \n",
            "petrain 1, epoch 1, train acc  0.7908 loss  0.7770, val acc  0.8734, loss  0.4992 \n",
            "petrain 1, epoch 2, train acc  0.7854 loss  0.7507, val acc  0.8532, loss  0.5229 \n",
            "petrain 1, epoch 3, train acc  0.7926 loss  0.7409, val acc  0.8566, loss  0.5063 \n",
            "petrain 1, epoch 4, train acc  0.7940 loss  0.7283, val acc  0.8828, loss  0.4549 \n",
            "petrain 1, epoch 5, train acc  0.7756 loss  0.7436, val acc  0.8652, loss  0.4806 \n",
            "petrain 1, epoch 6, train acc  0.8070 loss  0.6631, val acc  0.8666, loss  0.4701 \n",
            "petrain 1, epoch 7, train acc  0.7966 loss  0.6930, val acc  0.8730, loss  0.4779 \n",
            "petrain 1, epoch 8, train acc  0.7940 loss  0.6891, val acc  0.8638, loss  0.4637 \n",
            "petrain 1, epoch 9, train acc  0.7940 loss  0.6832, val acc  0.8646, loss  0.4652 \n",
            "petrain 1, epoch 10, train acc  0.7926 loss  0.6811, val acc  0.8446, loss  0.4813 \n",
            "petrain 1, epoch 11, train acc  0.7950 loss  0.6804, val acc  0.8524, loss  0.4910 \n",
            "petrain 1, epoch 12, train acc  0.8010 loss  0.6526, val acc  0.8634, loss  0.4708 \n",
            "petrain 1, epoch 13, train acc  0.7946 loss  0.6664, val acc  0.8544, loss  0.4816 \n",
            "petrain 1, epoch 14, train acc  0.7966 loss  0.6749, val acc  0.8558, loss  0.4691 \n",
            "petrain 1, epoch 15, train acc  0.8040 loss  0.6366, val acc  0.8536, loss  0.4547 \n",
            "petrain 1, epoch 16, train acc  0.8016 loss  0.6389, val acc  0.8560, loss  0.4638 \n",
            "petrain 1, epoch 17, train acc  0.7946 loss  0.6429, val acc  0.8712, loss  0.4621 \n",
            "petrain 1, epoch 18, train acc  0.8086 loss  0.6241, val acc  0.8710, loss  0.4299 \n",
            "petrain 1, epoch 19, train acc  0.8028 loss  0.6390, val acc  0.8764, loss  0.4101 \n",
            "petrain 1, epoch 20, train acc  0.7948 loss  0.6530, val acc  0.8570, loss  0.4404 \n",
            "petrain 1, epoch 21, train acc  0.7984 loss  0.6347, val acc  0.8498, loss  0.4839 \n",
            "petrain 1, epoch 22, train acc  0.8022 loss  0.6237, val acc  0.8654, loss  0.4435 \n",
            "petrain 1, epoch 23, train acc  0.8038 loss  0.6358, val acc  0.8494, loss  0.4690 \n",
            "petrain 1, epoch 24, train acc  0.8020 loss  0.6206, val acc  0.8618, loss  0.4547 \n",
            "1on0_correct len:   13118, correlation:  0.9858\n",
            "1on0_wrong   len:    1877, correlation:  0.2610\n",
            "0on1_correct len:   11950, correlation:  0.9962\n",
            "0on1_wrong   len:    3045, correlation: -0.0588\n",
            "\n",
            "######################\n",
            "Create New Groups\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train acc  0.4596  0.3406 loss  1.8179921  1.9929313 val acc  0.6828194, loss  1.3322683\n",
            "epoch 1, train acc  0.5740  0.4718 loss  1.5146581  1.7406056 val acc  0.6692030, loss  1.3310224\n",
            "epoch 2, train acc  0.5937  0.5100 loss  1.4414340  1.6472063 val acc  0.6383660, loss  1.3778105\n",
            "epoch 3, train acc  0.6279  0.5282 loss  1.3556687  1.5793882 val acc  0.6812175, loss  1.3680564\n",
            "epoch 4, train acc  0.6503  0.5584 loss  1.2824488  1.4928328 val acc  0.6760112, loss  1.2544344\n",
            "epoch 5, train acc  0.6614  0.5716 loss  1.2194790  1.4462948 val acc  0.7184622, loss  1.0311992\n",
            "epoch 6, train acc  0.6685  0.5822 loss  1.1941990  1.4119695 val acc  0.7112535, loss  1.1233728\n",
            "epoch 7, train acc  0.6723  0.5878 loss  1.1605670  1.3632592 val acc  0.7272727, loss  1.0293395\n",
            "epoch 8, train acc  0.6839  0.6008 loss  1.1164706  1.3052721 val acc  0.7052463, loss  1.1361206\n",
            "epoch 9, train acc  0.6913  0.6090 loss  1.0869508  1.2905104 val acc  0.7252703, loss  1.0366931\n",
            "epoch 10, train acc  0.6959  0.6132 loss  1.0509893  1.2734358 val acc  0.7200641, loss  1.0315314\n",
            "epoch 11, train acc  0.7061  0.6274 loss  1.0060001  1.2041819 val acc  0.7336804, loss  0.9797669\n",
            "epoch 12, train acc  0.7206  0.6392 loss  0.9475341  1.1679415 val acc  0.7288747, loss  0.9558133\n",
            "epoch 13, train acc  0.7237  0.6424 loss  0.9361569  1.1325096 val acc  0.7224669, loss  1.0434531\n",
            "epoch 14, train acc  0.7303  0.6494 loss  0.9002078  1.1186268 val acc  0.7388867, loss  0.9171332\n",
            "epoch 15, train acc  0.7334  0.6618 loss  0.8851780  1.0653777 val acc  0.7296756, loss  0.9318108\n",
            "epoch 16, train acc  0.7444  0.6780 loss  0.8334936  1.0147160 val acc  0.7260713, loss  0.9523681\n",
            "epoch 17, train acc  0.7426  0.6770 loss  0.8339333  1.0173867 val acc  0.7380857, loss  0.9437054\n",
            "epoch 18, train acc  0.7568  0.6864 loss  0.7781235  0.9777280 val acc  0.7372848, loss  0.9108995\n",
            "epoch 19, train acc  0.7646  0.7000 loss  0.7555298  0.9253481 val acc  0.7436925, loss  0.8514991\n",
            "epoch 20, train acc  0.7678  0.7056 loss  0.7355782  0.9067479 val acc  0.7432920, loss  0.8937440\n",
            "epoch 21, train acc  0.7671  0.7060 loss  0.7256638  0.9039542 val acc  0.7496997, loss  0.8677992\n",
            "epoch 22, train acc  0.7794  0.7212 loss  0.7024856  0.8740899 val acc  0.7424910, loss  0.8126578\n",
            "epoch 23, train acc  0.7814  0.7212 loss  0.6839818  0.8455155 val acc  0.7537044, loss  0.8512748\n",
            "epoch 24, train acc  0.7812  0.7198 loss  0.6630647  0.8187796 val acc  0.7448939, loss  0.8145649\n",
            "epoch 25, train acc  0.7877  0.7260 loss  0.6475403  0.8187702 val acc  0.7432920, loss  0.8411433\n",
            "epoch 26, train acc  0.7933  0.7316 loss  0.6333339  0.7892786 val acc  0.7404886, loss  0.8810001\n",
            "epoch 27, train acc  0.8025  0.7470 loss  0.6094594  0.7576705 val acc  0.7408891, loss  0.8870926\n",
            "epoch 28, train acc  0.8086  0.7566 loss  0.5896180  0.7496451 val acc  0.7456948, loss  0.8403623\n",
            "epoch 29, train acc  0.8059  0.7548 loss  0.5881463  0.7321851 val acc  0.7404886, loss  0.8649240\n",
            "epoch 30, train acc  0.8143  0.7624 loss  0.5681106  0.7162835 val acc  0.7488987, loss  0.8197032\n",
            "epoch 31, train acc  0.8089  0.7520 loss  0.5756856  0.7268574 val acc  0.7537044, loss  0.8284360\n",
            "epoch 32, train acc  0.8206  0.7742 loss  0.5444621  0.6785858 val acc  0.7621145, loss  0.7993743\n",
            "epoch 33, train acc  0.8240  0.7746 loss  0.5303890  0.6736352 val acc  0.7505006, loss  0.8254095\n",
            "epoch 34, train acc  0.8320  0.7768 loss  0.5077166  0.6569763 val acc  0.7480977, loss  0.8293505\n",
            "epoch 35, train acc  0.8269  0.7784 loss  0.5235436  0.6541871 val acc  0.7561073, loss  0.8392464\n",
            "epoch 36, train acc  0.8349  0.7872 loss  0.5008166  0.6350593 val acc  0.7501001, loss  0.8271175\n",
            "epoch 37, train acc  0.8315  0.7812 loss  0.4934844  0.6274304 val acc  0.7645174, loss  0.8027539\n",
            "epoch 38, train acc  0.8369  0.7930 loss  0.4851065  0.6130502 val acc  0.7392871, loss  0.8711805\n",
            "epoch 39, train acc  0.8422  0.7970 loss  0.4731388  0.6096015 val acc  0.7557068, loss  0.8668774\n",
            "epoch 40, train acc  0.8430  0.7988 loss  0.4673394  0.5970659 val acc  0.7597117, loss  0.8564881\n",
            "epoch 41, train acc  0.8480  0.7998 loss  0.4551162  0.5840927 val acc  0.7585102, loss  0.8323486\n",
            "epoch 42, train acc  0.8390  0.7934 loss  0.4731163  0.6037497 val acc  0.7529035, loss  0.8728929\n",
            "epoch 43, train acc  0.8448  0.7982 loss  0.4674123  0.5988574 val acc  0.7633160, loss  0.8606294\n",
            "epoch 44, train acc  0.8495  0.8028 loss  0.4445529  0.5605692 val acc  0.7525030, loss  0.8557999\n",
            "epoch 45, train acc  0.8514  0.8042 loss  0.4384426  0.5677022 val acc  0.7613136, loss  0.8480575\n",
            "epoch 46, train acc  0.8515  0.8064 loss  0.4336971  0.5570503 val acc  0.7605127, loss  0.8520713\n",
            "epoch 47, train acc  0.8598  0.8100 loss  0.4164561  0.5453705 val acc  0.7573088, loss  0.8793304\n",
            "epoch 48, train acc  0.8595  0.8202 loss  0.4173822  0.5271781 val acc  0.7581097, loss  0.8331851\n",
            "epoch 49, train acc  0.8608  0.8178 loss  0.4152450  0.5266297 val acc  0.7561073, loss  0.8331781\n",
            "epoch 50, train acc  0.8631  0.8188 loss  0.4057583  0.5290085 val acc  0.7513016, loss  0.8739637\n",
            "epoch 51, train acc  0.8651  0.8250 loss  0.3982646  0.5064528 val acc  0.7549059, loss  0.8453096\n",
            "epoch 52, train acc  0.8651  0.8210 loss  0.3944113  0.5119913 val acc  0.7577093, loss  0.8566661\n",
            "epoch 53, train acc  0.8655  0.8238 loss  0.3961528  0.5129448 val acc  0.7569083, loss  0.8220010\n",
            "epoch 54, train acc  0.8655  0.8216 loss  0.3894035  0.5039377 val acc  0.7657189, loss  0.8529415\n",
            "epoch 55, train acc  0.8699  0.8348 loss  0.3820351  0.4984203 val acc  0.7577093, loss  0.8510191\n",
            "epoch 56, train acc  0.8705  0.8270 loss  0.3764644  0.4853371 val acc  0.7617140, loss  0.8243677\n",
            "epoch 57, train acc  0.8732  0.8340 loss  0.3792021  0.4868272 val acc  0.7621145, loss  0.8154029\n",
            "epoch 58, train acc  0.8724  0.8238 loss  0.3737383  0.5004170 val acc  0.7665198, loss  0.8152893\n",
            "epoch 59, train acc  0.8749  0.8380 loss  0.3677905  0.4735298 val acc  0.7496997, loss  0.8789878\n",
            "epoch 60, train acc  0.8706  0.8346 loss  0.3728997  0.4836278 val acc  0.7561073, loss  0.8625492\n",
            "epoch 61, train acc  0.8765  0.8338 loss  0.3628244  0.4750459 val acc  0.7753304, loss  0.8035128\n",
            "epoch 62, train acc  0.8790  0.8416 loss  0.3556199  0.4658076 val acc  0.7617140, loss  0.8344143\n",
            "epoch 63, train acc  0.8796  0.8362 loss  0.3534757  0.4657321 val acc  0.7633160, loss  0.8383274\n",
            "epoch 64, train acc  0.8823  0.8388 loss  0.3511577  0.4569999 val acc  0.7613136, loss  0.8276547\n",
            "epoch 65, train acc  0.8869  0.8456 loss  0.3394691  0.4468998 val acc  0.7581097, loss  0.8319851\n",
            "epoch 66, train acc  0.8747  0.8308 loss  0.3642624  0.4747486 val acc  0.7661194, loss  0.8136237\n",
            "epoch 67, train acc  0.8831  0.8480 loss  0.3487023  0.4468420 val acc  0.7657189, loss  0.8212228\n",
            "epoch 68, train acc  0.8861  0.8498 loss  0.3356231  0.4335011 val acc  0.7613136, loss  0.8703147\n",
            "epoch 69, train acc  0.8889  0.8476 loss  0.3330202  0.4404925 val acc  0.7693232, loss  0.8409388\n",
            "epoch 70, train acc  0.8891  0.8458 loss  0.3287503  0.4361136 val acc  0.7593112, loss  0.8932676\n",
            "epoch 71, train acc  0.8863  0.8444 loss  0.3402331  0.4531302 val acc  0.7597117, loss  0.8762488\n",
            "epoch 72, train acc  0.8889  0.8480 loss  0.3336589  0.4415568 val acc  0.7593112, loss  0.8836785\n",
            "epoch 73, train acc  0.8860  0.8442 loss  0.3341119  0.4401339 val acc  0.7709251, loss  0.8631062\n",
            "epoch 74, train acc  0.8884  0.8448 loss  0.3311095  0.4369757 val acc  0.7701241, loss  0.8453846\n",
            "epoch 75, train acc  0.8905  0.8488 loss  0.3224785  0.4263131 val acc  0.7649179, loss  0.8215313\n",
            "epoch 76, train acc  0.8877  0.8472 loss  0.3320413  0.4390818 val acc  0.7641169, loss  0.8264598\n",
            "epoch 77, train acc  0.8929  0.8454 loss  0.3231987  0.4361181 val acc  0.7689227, loss  0.8278208\n",
            "epoch 78, train acc  0.8903  0.8524 loss  0.3218083  0.4244356 val acc  0.7681218, loss  0.8412615\n",
            "epoch 79, train acc  0.8918  0.8540 loss  0.3194338  0.4238120 val acc  0.7625150, loss  0.8514692\n",
            "epoch 80, train acc  0.8935  0.8522 loss  0.3151564  0.4208738 val acc  0.7641169, loss  0.8425824\n",
            "epoch 81, train acc  0.8961  0.8586 loss  0.3137488  0.4118542 val acc  0.7669203, loss  0.8403039\n",
            "Best train\n",
            "{'worst_loss': 0.4118542185425758, 'avg_loss': 0.3137487878650427, 'worst_acc': 0.858599973320961, 'avg_acc': 0.8961499769985676}\n",
            "Best val\n",
            "{'acc': 0.7753304243087769, 'loss': 0.8035127860307694}\n",
            "Test\n",
            "{'acc': 0.4361233413219452, 'loss': 2.3569690680503843}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_acc_values['pi'] = test_res['acc'] #pi = predict interpolate - their method\n",
        "dict_loss_values['pi'] = test_res['loss']"
      ],
      "metadata": {
        "id": "VnT37xv4VeTo"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparision of different baselines"
      ],
      "metadata": {
        "id": "4FDqXchFTcoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_acc_values"
      ],
      "metadata": {
        "id": "VFdA4d4GMPMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25399798-ed53-4a2e-b7fc-773a3da2de15"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'erm': 0.19743692874908447,\n",
              " 'irm': 0.17140568792819977,\n",
              " 'dro': 0.24789747595787048,\n",
              " 'oracle': 0.43412095308303833,\n",
              " 'rgm': 0.20104125142097473,\n",
              " 'pi': 0.4361233413219452}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_loss_values"
      ],
      "metadata": {
        "id": "ultTKAAGMQBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704eb3be-12d3-4976-da1f-ec1b250bd8b8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'erm': 2.4162051916122436,\n",
              " 'irm': 2.5557396364212037,\n",
              " 'dro': 2.1978156423568724,\n",
              " 'oracle': 0.43412095308303833,\n",
              " 'rgm': 0.20104125142097473,\n",
              " 'pi': 2.3569690680503843}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot for accuracy for different methods**"
      ],
      "metadata": {
        "id": "afdt1LYcUBnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(dict_acc_values.keys())\n",
        "values = list(dict_acc_values.values())\n",
        "\n",
        "plt.bar(labels, values)\n",
        "plt.title(\"Accuracy values\")\n",
        "plt.xlabel(\"Methods\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wpU1PUitMRBN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "020d00f1-88ec-451c-d3a8-6e423836a687"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(dict_loss_values.keys())\n",
        "values = list(dict_loss_values.values())\n",
        "\n",
        "plt.bar(labels, values)\n",
        "plt.title(\"Loss values\")\n",
        "plt.xlabel(\"Methods\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PXBdeCAshXVy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "eac63455-d3f6-4357-be33-c2c85c4fba37"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}